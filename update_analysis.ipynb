{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de482d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# general imports\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import numpy.typing as npt\n",
    "import os\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "from functools import singledispatch\n",
    "\n",
    "# dataset\n",
    "from sklearn.datasets import fetch_kddcup99\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# pyspark module\n",
    "from pyspark.rdd import RDD\n",
    "\n",
    "# src module\n",
    "from src.utils import kddSetup, sparkSetup\n",
    "from src.kmeans import compute_centroidDistances, get_clusterId, get_minDistance, compute_cost, \\\n",
    "    kMeansParallel_init, kMeansPlusPlus_init, kMeansRandom_init, \\\n",
    "    lloydKMeans, miniBatchKMeans"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afad672d",
   "metadata": {},
   "source": [
    "Starting the cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9fc77437",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the zipped environment if it doesn't already exist\n",
    "! if [ ! -f \"environment.tar.gz\" ]; then venv-pack -o \"environment.tar.gz\" ; fi\n",
    "# creating the zipped module src\n",
    "! if [ -f \"src.tar.gz\" ]; then rm src.tar.gz ; fi\n",
    "! tar -czf src.tar.gz src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cc6dba2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting org.apache.spark.deploy.master.Master, logging to /usr/local/spark/logs/spark-ubuntu-org.apache.spark.deploy.master.Master-1-mapd-b-14-1.out\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "worker3: starting org.apache.spark.deploy.worker.Worker, logging to /usr/local/spark/logs/spark-ubuntu-org.apache.spark.deploy.worker.Worker-1-mapd-b-14-4.out\n",
      "master: starting org.apache.spark.deploy.worker.Worker, logging to /usr/local/spark/logs/spark-ubuntu-org.apache.spark.deploy.worker.Worker-1-mapd-b-14-1.out\n",
      "worker1: starting org.apache.spark.deploy.worker.Worker, logging to /usr/local/spark/logs/spark-ubuntu-org.apache.spark.deploy.worker.Worker-1-mapd-b-14-2.out\n",
      "worker2: starting org.apache.spark.deploy.worker.Worker, logging to /usr/local/spark/logs/spark-ubuntu-org.apache.spark.deploy.worker.Worker-1-mapd-b-14-3.out\n"
     ]
    }
   ],
   "source": [
    "# starting the cluster\n",
    "! $SPARK_HOME/sbin/start-all.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c8d733ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"PYSPARK_PYTHON\"] = \"./environment/bin/python\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3a075367",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/09/12 08:06:18 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/09/12 08:06:25 WARN Utils: Untarring behavior will be deprecated at spark.files and SparkContext.addFile. Consider using spark.archives or SparkContext.addArchive instead.\n"
     ]
    }
   ],
   "source": [
    "# creating a sparkSession\n",
    "spark = sparkSetup(\"kMeans\")\n",
    "sc = spark.sparkContext\n",
    "# exporting the src module\n",
    "sc.addPyFile(\"src.tar.gz\") # telling spark where to find the python binary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e086be1b",
   "metadata": {},
   "source": [
    "Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3e68db5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "kdd_data, kdd_labels, entries_dict = kddSetup(standardize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9bb0b658",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = np.unique(kdd_labels).shape[0]\n",
    "l = 0.5 * k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4493579f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/09/12 08:07:36 WARN TaskSetManager: Stage 0 contains a task of very large size (10603 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/09/12 08:07:39 WARN TaskSetManager: Stage 1 contains a task of very large size (10603 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/09/12 08:07:40 WARN TaskSetManager: Stage 2 contains a task of very large size (10603 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/09/12 08:07:41 WARN TaskSetManager: Stage 3 contains a task of very large size (10603 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/09/12 08:07:42 WARN TaskSetManager: Stage 4 contains a task of very large size (10603 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/09/12 08:07:44 WARN TaskSetManager: Stage 5 contains a task of very large size (10603 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/09/12 08:07:45 WARN TaskSetManager: Stage 6 contains a task of very large size (10603 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/09/12 08:07:46 WARN TaskSetManager: Stage 7 contains a task of very large size (10603 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/09/12 08:07:47 WARN TaskSetManager: Stage 8 contains a task of very large size (10603 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/09/12 08:07:49 WARN TaskSetManager: Stage 9 contains a task of very large size (10603 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/09/12 08:07:50 WARN TaskSetManager: Stage 10 contains a task of very large size (10603 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/09/12 08:07:51 WARN TaskSetManager: Stage 11 contains a task of very large size (10603 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/09/12 08:07:52 WARN TaskSetManager: Stage 12 contains a task of very large size (10603 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/09/12 08:07:54 WARN TaskSetManager: Stage 13 contains a task of very large size (10603 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/09/12 08:07:55 WARN TaskSetManager: Stage 14 contains a task of very large size (10603 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/09/12 08:07:56 WARN TaskSetManager: Stage 15 contains a task of very large size (10603 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/09/12 08:07:57 WARN TaskSetManager: Stage 16 contains a task of very large size (10603 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/09/12 08:07:59 WARN TaskSetManager: Stage 17 contains a task of very large size (10603 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/09/12 08:07:59 WARN TaskSetManager: Stage 18 contains a task of very large size (10603 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/09/12 08:08:01 WARN TaskSetManager: Stage 19 contains a task of very large size (10603 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/09/12 08:08:02 WARN TaskSetManager: Stage 20 contains a task of very large size (10603 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/09/12 08:08:04 WARN TaskSetManager: Stage 21 contains a task of very large size (10603 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/09/12 08:08:04 WARN TaskSetManager: Stage 22 contains a task of very large size (10603 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/09/12 08:08:06 WARN TaskSetManager: Stage 23 contains a task of very large size (10603 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/09/12 08:08:07 WARN TaskSetManager: Stage 24 contains a task of very large size (10603 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/09/12 08:08:09 WARN TaskSetManager: Stage 25 contains a task of very large size (10603 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/09/12 08:08:10 WARN TaskSetManager: Stage 26 contains a task of very large size (10603 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/09/12 08:08:11 WARN TaskSetManager: Stage 27 contains a task of very large size (10603 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/09/12 08:08:12 WARN TaskSetManager: Stage 28 contains a task of very large size (10603 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/09/12 08:08:14 WARN TaskSetManager: Stage 29 contains a task of very large size (10603 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/09/12 08:08:15 WARN TaskSetManager: Stage 30 contains a task of very large size (10603 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/09/12 08:08:16 WARN TaskSetManager: Stage 31 contains a task of very large size (10603 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/09/12 08:08:17 WARN TaskSetManager: Stage 32 contains a task of very large size (10603 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/09/12 08:08:19 WARN TaskSetManager: Stage 33 contains a task of very large size (10603 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/09/12 08:08:20 WARN TaskSetManager: Stage 34 contains a task of very large size (10603 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/09/12 08:08:22 WARN TaskSetManager: Stage 35 contains a task of very large size (10603 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/09/12 08:08:22 WARN TaskSetManager: Stage 36 contains a task of very large size (10603 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/09/12 08:08:24 WARN TaskSetManager: Stage 37 contains a task of very large size (10603 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ParallelCollectionRDD[0] at readRDDFromFile at PythonRDD.scala:289"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_rdd = sc.parallelize([row for row in kdd_data])\n",
    "data_rdd = data_rdd.persist()\n",
    "centroids = kMeansParallel_init(data_rdd, k, l)\n",
    "data_rdd.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5ff3e80d",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_dict = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b73df2",
   "metadata": {},
   "source": [
    "Lloyd's kMeans serial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2cfa59fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "final_centroids = lloydKMeans(kdd_data, centroids, epochs = 10)\n",
    "delta_time = time.time() - start_time\n",
    "time_dict[1] = {\"lloyd\": delta_time}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db70758a",
   "metadata": {},
   "source": [
    "Parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bf08e279",
   "metadata": {},
   "outputs": [],
   "source": [
    "partitions = [2, 4, 8, 16]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "63ee151f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: {'lloyd': 42.844629526138306}}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "92be6a27",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/09/12 08:14:20 WARN TaskSetManager: Stage 42 contains a task of very large size (10603 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/09/12 08:14:21 ERROR Inbox: An error happened while processing message in the inbox for CoarseGrainedScheduler\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "Exception in thread \"dispatcher-CoarseGrainedScheduler\" java.lang.OutOfMemoryError: Java heap space\n",
      "ERROR:root:KeyboardInterrupt while sending command.===>           (4 + 11) / 16]\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3.12/socket.py\", line 707, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m#start_time = time.time()\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m#final_centroids = lloydKMeans(data_rdd, centroids, epochs = 10)\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m#delta_time = time.time() - start_time\u001b[39;00m\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m#time_dict[numSlices][\"lloyd\"] = delta_time\u001b[39;00m\n\u001b[32m     10\u001b[39m start_time = time.time()\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m final_centroids = \u001b[43mminiBatchKMeans\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_rdd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcentroids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     12\u001b[39m delta_time = time.time() - start_time\n\u001b[32m     13\u001b[39m time_dict[numSlices][\u001b[33m\"\u001b[39m\u001b[33mminiBatch\u001b[39m\u001b[33m\"\u001b[39m] = delta_time\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Distributed-K-means/src/kmeans.py:342\u001b[39m, in \u001b[36mminiBatchKMeans\u001b[39m\u001b[34m(data_rdd, centroids, epochs, batch_fraction)\u001b[39m\n\u001b[32m    336\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m \u001b[38;5;28miter\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[32m    337\u001b[39m     miniBatch_rdd = data_rdd \\\n\u001b[32m    338\u001b[39m         .sample(withReplacement=\u001b[38;5;28;01mFalse\u001b[39;00m, fraction=batch_fraction)\n\u001b[32m    339\u001b[39m     clusterMetrics = \u001b[38;5;28mdict\u001b[39m(\u001b[43mminiBatch_rdd\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[32m    340\u001b[39m \u001b[43m        \u001b[49m\u001b[43m.\u001b[49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mget_clusterId\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcompute_centroidDistances\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcentroids\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[32m    341\u001b[39m \u001b[43m        \u001b[49m\u001b[43m.\u001b[49m\u001b[43mreduceByKey\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[32m--> \u001b[39m\u001b[32m342\u001b[39m \u001b[43m        \u001b[49m\u001b[43m.\u001b[49m\u001b[43mcollect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    343\u001b[39m     )\n\u001b[32m    344\u001b[39m     \u001b[38;5;66;03m# edge case in which a centroid has no assignments\u001b[39;00m\n\u001b[32m    345\u001b[39m     \u001b[38;5;66;03m# if its also the first iteration: repeat the iteration\u001b[39;00m\n\u001b[32m    346\u001b[39m     state = \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/spark/python/pyspark/rdd.py:1833\u001b[39m, in \u001b[36mRDD.collect\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1831\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m SCCallSiteSync(\u001b[38;5;28mself\u001b[39m.context):\n\u001b[32m   1832\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.ctx._jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1833\u001b[39m     sock_info = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mctx\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_jvm\u001b[49m\u001b[43m.\u001b[49m\u001b[43mPythonRDD\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcollectAndServe\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_jrdd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrdd\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1834\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(_load_from_socket(sock_info, \u001b[38;5;28mself\u001b[39m._jrdd_deserializer))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1321\u001b[39m, in \u001b[36mJavaMember.__call__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m   1314\u001b[39m args_command, temp_args = \u001b[38;5;28mself\u001b[39m._build_args(*args)\n\u001b[32m   1316\u001b[39m command = proto.CALL_COMMAND_NAME +\\\n\u001b[32m   1317\u001b[39m     \u001b[38;5;28mself\u001b[39m.command_header +\\\n\u001b[32m   1318\u001b[39m     args_command +\\\n\u001b[32m   1319\u001b[39m     proto.END_COMMAND_PART\n\u001b[32m-> \u001b[39m\u001b[32m1321\u001b[39m answer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgateway_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1322\u001b[39m return_value = get_return_value(\n\u001b[32m   1323\u001b[39m     answer, \u001b[38;5;28mself\u001b[39m.gateway_client, \u001b[38;5;28mself\u001b[39m.target_id, \u001b[38;5;28mself\u001b[39m.name)\n\u001b[32m   1325\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1038\u001b[39m, in \u001b[36mGatewayClient.send_command\u001b[39m\u001b[34m(self, command, retry, binary)\u001b[39m\n\u001b[32m   1036\u001b[39m connection = \u001b[38;5;28mself\u001b[39m._get_connection()\n\u001b[32m   1037\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1038\u001b[39m     response = \u001b[43mconnection\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1039\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m binary:\n\u001b[32m   1040\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m response, \u001b[38;5;28mself\u001b[39m._create_connection_guard(connection)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:511\u001b[39m, in \u001b[36mClientServerConnection.send_command\u001b[39m\u001b[34m(self, command)\u001b[39m\n\u001b[32m    509\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    510\u001b[39m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m511\u001b[39m         answer = smart_decode(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m[:-\u001b[32m1\u001b[39m])\n\u001b[32m    512\u001b[39m         logger.debug(\u001b[33m\"\u001b[39m\u001b[33mAnswer received: \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[33m\"\u001b[39m.format(answer))\n\u001b[32m    513\u001b[39m         \u001b[38;5;66;03m# Happens when a the other end is dead. There might be an empty\u001b[39;00m\n\u001b[32m    514\u001b[39m         \u001b[38;5;66;03m# answer before the socket raises an error.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/socket.py:707\u001b[39m, in \u001b[36mSocketIO.readinto\u001b[39m\u001b[34m(self, b)\u001b[39m\n\u001b[32m    705\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m    706\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m707\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sock\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    708\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[32m    709\u001b[39m         \u001b[38;5;28mself\u001b[39m._timeout_occurred = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 40:=>              (1 + 15) / 16][Stage 42:====>           (4 + 11) / 16]\r"
     ]
    }
   ],
   "source": [
    "numSlices = 16\n",
    "time_dict[numSlices] = {}\n",
    "data_rdd = sc.parallelize([row for row in kdd_data], numSlices=numSlices).persist()\n",
    "\n",
    "start_time = time.time()\n",
    "final_centroids = lloydKMeans(data_rdd, centroids, epochs = 10)\n",
    "delta_time = time.time() - start_time\n",
    "time_dict[numSlices][\"lloyd\"] = delta_time\n",
    "\n",
    "start_time = time.time()\n",
    "final_centroids = miniBatchKMeans(data_rdd, centroids, epochs = 10)\n",
    "delta_time = time.time() - start_time\n",
    "time_dict[numSlices][\"miniBatch\"] = delta_time\n",
    "\n",
    "data_rdd.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "be94e0a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/09/11 14:58:09 WARN TaskSetManager: Stage 38 contains a task of very large size (85120 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/09/11 14:58:10 ERROR Inbox: An error happened while processing message in the inbox for CoarseGrainedScheduler\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.base/java.util.Arrays.copyOf(Arrays.java:3537)\n",
      "\tat java.base/java.io.ByteArrayOutputStream.ensureCapacity(ByteArrayOutputStream.java:100)\n",
      "\tat java.base/java.io.ByteArrayOutputStream.write(ByteArrayOutputStream.java:130)\n",
      "\tat org.apache.spark.util.ByteBufferOutputStream.write(ByteBufferOutputStream.scala:41)\n",
      "\tat java.base/java.io.DataOutputStream.write(DataOutputStream.java:112)\n",
      "\tat java.base/java.io.ObjectOutputStream$BlockDataOutputStream.write(ObjectOutputStream.java:1862)\n",
      "\tat java.base/java.io.ObjectOutputStream.write(ObjectOutputStream.java:714)\n",
      "\tat java.base/java.nio.channels.Channels$WritableByteChannelImpl.write(Channels.java:463)\n",
      "\tat org.apache.spark.util.SerializableBuffer.$anonfun$writeObject$1(SerializableBuffer.scala:49)\n",
      "\tat org.apache.spark.util.SerializableBuffer$$Lambda$1508/0x000072839079b800.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryOrIOException(SparkErrorUtils.scala:35)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryOrIOException$(SparkErrorUtils.scala:33)\n",
      "\tat org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:94)\n",
      "\tat org.apache.spark.util.SerializableBuffer.writeObject(SerializableBuffer.scala:47)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n",
      "\tat java.base/java.io.ObjectStreamClass.invokeWriteObject(ObjectStreamClass.java:1070)\n",
      "\tat java.base/java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1516)\n",
      "\tat java.base/java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1438)\n",
      "\tat java.base/java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1181)\n",
      "\tat java.base/java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1572)\n",
      "\tat java.base/java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1529)\n",
      "\tat java.base/java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1438)\n",
      "\tat java.base/java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1181)\n",
      "\tat java.base/java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:350)\n",
      "\tat org.apache.spark.serializer.JavaSerializationStream.writeObject(JavaSerializer.scala:46)\n",
      "\tat org.apache.spark.rpc.netty.RequestMessage.serialize(NettyRpcEnv.scala:600)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.send(NettyRpcEnv.scala:199)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEndpointRef.send(NettyRpcEnv.scala:563)\n",
      "\tat org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint.$anonfun$launchTasks$1(CoarseGrainedSchedulerBackend.scala:443)\n",
      "Exception in thread \"dispatcher-CoarseGrainedScheduler\" java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.base/java.util.Arrays.copyOf(Arrays.java:3537)\n",
      "\tat java.base/java.io.ByteArrayOutputStream.ensureCapacity(ByteArrayOutputStream.java:100)\n",
      "\tat java.base/java.io.ByteArrayOutputStream.write(ByteArrayOutputStream.java:130)\n",
      "\tat org.apache.spark.util.ByteBufferOutputStream.write(ByteBufferOutputStream.scala:41)\n",
      "\tat java.base/java.io.DataOutputStream.write(DataOutputStream.java:112)\n",
      "\tat java.base/java.io.ObjectOutputStream$BlockDataOutputStream.write(ObjectOutputStream.java:1862)\n",
      "\tat java.base/java.io.ObjectOutputStream.write(ObjectOutputStream.java:714)\n",
      "\tat java.base/java.nio.channels.Channels$WritableByteChannelImpl.write(Channels.java:463)\n",
      "\tat org.apache.spark.util.SerializableBuffer.$anonfun$writeObject$1(SerializableBuffer.scala:49)\n",
      "\tat org.apache.spark.util.SerializableBuffer$$Lambda$1508/0x000072839079b800.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryOrIOException(SparkErrorUtils.scala:35)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryOrIOException$(SparkErrorUtils.scala:33)\n",
      "\tat org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:94)\n",
      "\tat org.apache.spark.util.SerializableBuffer.writeObject(SerializableBuffer.scala:47)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n",
      "\tat java.base/java.io.ObjectStreamClass.invokeWriteObject(ObjectStreamClass.java:1070)\n",
      "\tat java.base/java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1516)\n",
      "\tat java.base/java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1438)\n",
      "\tat java.base/java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1181)\n",
      "\tat java.base/java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1572)\n",
      "\tat java.base/java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1529)\n",
      "\tat java.base/java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1438)\n",
      "\tat java.base/java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1181)\n",
      "\tat java.base/java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:350)\n",
      "\tat org.apache.spark.serializer.JavaSerializationStream.writeObject(JavaSerializer.scala:46)\n",
      "\tat org.apache.spark.rpc.netty.RequestMessage.serialize(NettyRpcEnv.scala:600)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.send(NettyRpcEnv.scala:199)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEndpointRef.send(NettyRpcEnv.scala:563)\n",
      "\tat org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint.$anonfun$launchTasks$1(CoarseGrainedSchedulerBackend.scala:443)\n",
      "ERROR:root:KeyboardInterrupt while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3.12/socket.py\", line 707, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      3\u001b[39m data_rdd = sc.parallelize([row \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m kdd_data], numSlices=numSlices).persist()\n\u001b[32m      5\u001b[39m start_time = time.time()\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m final_centroids = \u001b[43mlloydKMeans\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_rdd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcentroids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      7\u001b[39m delta_time = time.time() - start_time\n\u001b[32m      8\u001b[39m time_dict[numSlices][\u001b[33m\"\u001b[39m\u001b[33mlloyd\u001b[39m\u001b[33m\"\u001b[39m] = delta_time\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/functools.py:909\u001b[39m, in \u001b[36msingledispatch.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kw)\u001b[39m\n\u001b[32m    905\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m args:\n\u001b[32m    906\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfuncname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m requires at least \u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m    907\u001b[39m                     \u001b[33m'\u001b[39m\u001b[33m1 positional argument\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m909\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdispatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__class__\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Distributed-K-means/src/kmeans.py:249\u001b[39m, in \u001b[36m_\u001b[39m\u001b[34m(data, centroids, epochs)\u001b[39m\n\u001b[32m    244\u001b[39m k = centroids.shape[\u001b[32m0\u001b[39m]\n\u001b[32m    245\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[32m    246\u001b[39m     clusterMetrics = \u001b[38;5;28mdict\u001b[39m(\u001b[43mdata\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[32m    247\u001b[39m \u001b[43m        \u001b[49m\u001b[43m.\u001b[49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mget_clusterId\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcompute_centroidDistances\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcentroids\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[32m    248\u001b[39m \u001b[43m        \u001b[49m\u001b[43m.\u001b[49m\u001b[43mreduceByKey\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[32m--> \u001b[39m\u001b[32m249\u001b[39m \u001b[43m        \u001b[49m\u001b[43m.\u001b[49m\u001b[43mcollect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    250\u001b[39m     )\n\u001b[32m    252\u001b[39m     \u001b[38;5;66;03m# # assign each point to the closest cluster\u001b[39;00m\n\u001b[32m    253\u001b[39m     \u001b[38;5;66;03m# data_assigned_rdd = data \\\u001b[39;00m\n\u001b[32m    254\u001b[39m     \u001b[38;5;66;03m#     .map(lambda x: (get_clusterId(compute_centroidDistances(x, centroids)), 1, x)) \\\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    269\u001b[39m     \u001b[38;5;66;03m# compute the weighted average (they are the updated clusters). \u001b[39;00m\n\u001b[32m    270\u001b[39m     \u001b[38;5;66;03m# If no counts maintain the older centroid values\u001b[39;00m\n\u001b[32m    271\u001b[39m     centroids = np.array(\n\u001b[32m    272\u001b[39m         [clusterMetrics[i][\u001b[32m1\u001b[39m]/clusterMetrics[i][\u001b[32m0\u001b[39m] \n\u001b[32m    273\u001b[39m             \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m clusterMetrics.keys() \u001b[38;5;28;01melse\u001b[39;00m centroids[i,:]\n\u001b[32m    274\u001b[39m                 \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(k)]\n\u001b[32m    275\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/spark/python/pyspark/rdd.py:1833\u001b[39m, in \u001b[36mRDD.collect\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1831\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m SCCallSiteSync(\u001b[38;5;28mself\u001b[39m.context):\n\u001b[32m   1832\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.ctx._jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1833\u001b[39m     sock_info = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mctx\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_jvm\u001b[49m\u001b[43m.\u001b[49m\u001b[43mPythonRDD\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcollectAndServe\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_jrdd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrdd\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1834\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(_load_from_socket(sock_info, \u001b[38;5;28mself\u001b[39m._jrdd_deserializer))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1321\u001b[39m, in \u001b[36mJavaMember.__call__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m   1314\u001b[39m args_command, temp_args = \u001b[38;5;28mself\u001b[39m._build_args(*args)\n\u001b[32m   1316\u001b[39m command = proto.CALL_COMMAND_NAME +\\\n\u001b[32m   1317\u001b[39m     \u001b[38;5;28mself\u001b[39m.command_header +\\\n\u001b[32m   1318\u001b[39m     args_command +\\\n\u001b[32m   1319\u001b[39m     proto.END_COMMAND_PART\n\u001b[32m-> \u001b[39m\u001b[32m1321\u001b[39m answer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgateway_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1322\u001b[39m return_value = get_return_value(\n\u001b[32m   1323\u001b[39m     answer, \u001b[38;5;28mself\u001b[39m.gateway_client, \u001b[38;5;28mself\u001b[39m.target_id, \u001b[38;5;28mself\u001b[39m.name)\n\u001b[32m   1325\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1038\u001b[39m, in \u001b[36mGatewayClient.send_command\u001b[39m\u001b[34m(self, command, retry, binary)\u001b[39m\n\u001b[32m   1036\u001b[39m connection = \u001b[38;5;28mself\u001b[39m._get_connection()\n\u001b[32m   1037\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1038\u001b[39m     response = \u001b[43mconnection\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1039\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m binary:\n\u001b[32m   1040\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m response, \u001b[38;5;28mself\u001b[39m._create_connection_guard(connection)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:511\u001b[39m, in \u001b[36mClientServerConnection.send_command\u001b[39m\u001b[34m(self, command)\u001b[39m\n\u001b[32m    509\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    510\u001b[39m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m511\u001b[39m         answer = smart_decode(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m[:-\u001b[32m1\u001b[39m])\n\u001b[32m    512\u001b[39m         logger.debug(\u001b[33m\"\u001b[39m\u001b[33mAnswer received: \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[33m\"\u001b[39m.format(answer))\n\u001b[32m    513\u001b[39m         \u001b[38;5;66;03m# Happens when a the other end is dead. There might be an empty\u001b[39;00m\n\u001b[32m    514\u001b[39m         \u001b[38;5;66;03m# answer before the socket raises an error.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/socket.py:707\u001b[39m, in \u001b[36mSocketIO.readinto\u001b[39m\u001b[34m(self, b)\u001b[39m\n\u001b[32m    705\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m    706\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m707\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sock\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    708\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[32m    709\u001b[39m         \u001b[38;5;28mself\u001b[39m._timeout_occurred = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "for numSlices in partitions:\n",
    "    time_dict[numSlices] = {}\n",
    "    data_rdd = sc.parallelize([row for row in kdd_data], numSlices=numSlices).persist()\n",
    "    \n",
    "    start_time = time.time()\n",
    "    final_centroids = lloydKMeans(data_rdd, centroids, epochs = 10)\n",
    "    delta_time = time.time() - start_time\n",
    "    time_dict[numSlices][\"lloyd\"] = delta_time\n",
    "\n",
    "    start_time = time.time()\n",
    "    final_centroids = miniBatchKMeans(data_rdd, centroids, epochs = 10)\n",
    "    delta_time = time.time() - start_time\n",
    "    time_dict[numSlices][\"miniBatch\"] = delta_time\n",
    "    \n",
    "    data_rdd.unpersist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1f5ab06",
   "metadata": {},
   "source": [
    "Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69800507",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ba66583",
   "metadata": {},
   "source": [
    "### Cost vs iterations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4fde6a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "iterations = np.linspace(10,50,5, dtype=int)\n",
    "results_dict = {\"lloyd_serial\":np.zeros(len(iterations)), \"lloyd_parallel\":np.zeros(len(iterations)), \"minibatch\":np.zeros(len(iterations))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d72960b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lloyd_serial\n",
      "running for 10 iterations...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 40:=>              (1 + 15) / 16][Stage 42:====>           (4 + 11) / 16]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running for 20 iterations...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 40:=>              (1 + 15) / 16][Stage 42:====>           (4 + 11) / 16]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running for 30 iterations...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 40:=>              (1 + 15) / 16][Stage 42:====>           (4 + 11) / 16]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running for 40 iterations...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 40:=>              (1 + 15) / 16][Stage 42:====>           (4 + 11) / 16]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running for 50 iterations...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[36]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mrunning for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00miterations[i]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m iterations...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m key == \u001b[33m'\u001b[39m\u001b[33mlloyd_serial\u001b[39m\u001b[33m'\u001b[39m:\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m     final_centroids = \u001b[43mlloydKMeans\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkdd_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43mcentroids\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mcentroids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43miterations\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      7\u001b[39m     algo_cost = compute_cost(kdd_data,final_centroids)\n\u001b[32m      8\u001b[39m     results_dict[key][i] = algo_cost\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/functools.py:909\u001b[39m, in \u001b[36msingledispatch.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kw)\u001b[39m\n\u001b[32m    905\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m args:\n\u001b[32m    906\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfuncname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m requires at least \u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m    907\u001b[39m                     \u001b[33m'\u001b[39m\u001b[33m1 positional argument\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m909\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdispatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__class__\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Distributed-K-means/src/kmeans.py:226\u001b[39m, in \u001b[36m_\u001b[39m\u001b[34m(data, centroids, epochs)\u001b[39m\n\u001b[32m    223\u001b[39m k = centroids.shape[\u001b[32m0\u001b[39m]\n\u001b[32m    224\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[32m    225\u001b[39m     assignments = np.array(\n\u001b[32m--> \u001b[39m\u001b[32m226\u001b[39m         [get_clusterId(\u001b[43mcompute_centroidDistances\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcentroids\u001b[49m\u001b[43m)\u001b[49m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m data]\n\u001b[32m    227\u001b[39m     )\n\u001b[32m    228\u001b[39m     centroids = np.array(\n\u001b[32m    229\u001b[39m         [np.mean(data[assignments==i,:], axis = \u001b[32m0\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m assignments \u001b[38;5;28;01melse\u001b[39;00m centroids[i,:] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(k)]\n\u001b[32m    230\u001b[39m     )\n\u001b[32m    231\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m centroids\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Distributed-K-means/src/kmeans.py:14\u001b[39m, in \u001b[36mcompute_centroidDistances\u001b[39m\u001b[34m(x, centroids)\u001b[39m\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcompute_centroidDistances\u001b[39m(\n\u001b[32m     11\u001b[39m     x: npt.NDArray, \n\u001b[32m     12\u001b[39m     centroids: npt.NDArray\n\u001b[32m     13\u001b[39m ) -> npt.NDArray:\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43msum\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcentroids\u001b[49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Distributed-K-means/.venv/lib/python3.12/site-packages/numpy/_core/fromnumeric.py:2466\u001b[39m, in \u001b[36msum\u001b[39m\u001b[34m(a, axis, dtype, out, keepdims, initial, where)\u001b[39m\n\u001b[32m   2463\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m out\n\u001b[32m   2464\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m res\n\u001b[32m-> \u001b[39m\u001b[32m2466\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_wrapreduction\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2467\u001b[39m \u001b[43m    \u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43madd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43msum\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2468\u001b[39m \u001b[43m    \u001b[49m\u001b[43mkeepdims\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkeepdims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minitial\u001b[49m\u001b[43m=\u001b[49m\u001b[43minitial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwhere\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwhere\u001b[49m\n\u001b[32m   2469\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Distributed-K-means/.venv/lib/python3.12/site-packages/numpy/_core/fromnumeric.py:86\u001b[39m, in \u001b[36m_wrapreduction\u001b[39m\u001b[34m(obj, ufunc, method, axis, dtype, out, **kwargs)\u001b[39m\n\u001b[32m     83\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     84\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m reduction(axis=axis, out=out, **passkwargs)\n\u001b[32m---> \u001b[39m\u001b[32m86\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mufunc\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreduce\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mpasskwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 40:=>              (1 + 15) / 16][Stage 42:====>           (4 + 11) / 16]\r"
     ]
    }
   ],
   "source": [
    "for key in results_dict.keys():\n",
    "    print(key)\n",
    "    for i in range(len(iterations)):\n",
    "        print(f\"running for {iterations[i]} iterations...\")\n",
    "        if key == 'lloyd_serial':\n",
    "            final_centroids = lloydKMeans(kdd_data,centroids = centroids, epochs = iterations[i])\n",
    "            algo_cost = compute_cost(kdd_data,final_centroids)\n",
    "            results_dict[key][i] = algo_cost\n",
    "        else:\n",
    "            break\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82c23915",
   "metadata": {},
   "source": [
    "Stopping the cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "501de77c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8ec63757",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "worker3: stopping org.apache.spark.deploy.worker.Worker\n",
      "worker1: stopping org.apache.spark.deploy.worker.Worker\n",
      "worker2: stopping org.apache.spark.deploy.worker.Worker\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/09/12 08:53:39 ERROR TaskSchedulerImpl: Lost executor 2 on 10.67.22.208: Worker shutting down\n",
      "25/09/12 08:53:39 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_57_15 !\n",
      "25/09/12 08:53:39 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_57_7 !\n",
      "25/09/12 08:53:39 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_57_3 !\n",
      "25/09/12 08:53:39 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_68_14 !\n",
      "25/09/12 08:53:39 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_57_11 !\n",
      "[Stage 40:=>              (1 + 14) / 16][Stage 42:====>            (4 + 8) / 16]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "master: stopping org.apache.spark.deploy.worker.Worker\n",
      "stopping org.apache.spark.deploy.master.Master\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/09/12 08:53:39 ERROR TaskSchedulerImpl: Lost executor 1 on 10.67.22.170: Command exited with code 143\n",
      "25/09/12 08:53:39 ERROR TaskSchedulerImpl: Lost executor 0 on 10.67.22.202: Command exited with code 143\n",
      "25/09/12 08:53:39 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_57_5 !\n",
      "25/09/12 08:53:39 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_57_9 !\n",
      "25/09/12 08:53:39 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_57_13 !\n",
      "25/09/12 08:53:39 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_68_12 !\n",
      "25/09/12 08:53:39 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_57_1 !\n",
      "25/09/12 08:53:39 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_57_8 !\n",
      "25/09/12 08:53:39 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_57_0 !\n",
      "25/09/12 08:53:39 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_68_15 !\n",
      "25/09/12 08:53:39 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_57_12 !\n",
      "25/09/12 08:53:39 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_57_4 !\n",
      "[Stage 40:=>               (1 + 8) / 16][Stage 42:====>            (4 + 2) / 16]\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/09/12 08:53:40 WARN StandaloneAppClient$ClientEndpoint: Connection to 10.67.22.224:7077 failed; waiting for master to reconnect...\n",
      "25/09/12 08:53:40 WARN StandaloneSchedulerBackend: Disconnected from Spark cluster! Waiting for reconnection...\n",
      "25/09/12 08:53:44 ERROR TaskSchedulerImpl: Lost executor 3 on 10.67.22.224: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.\n",
      "25/09/12 08:53:44 WARN TaskSetManager: Lost task 15.1 in stage 40.0 (TID 672) (10.67.22.224 executor 3): ExecutorLostFailure (executor 3 exited caused by one of the running tasks) Reason: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.\n",
      "25/09/12 08:53:44 WARN TaskSetManager: Lost task 7.1 in stage 40.0 (TID 673) (10.67.22.224 executor 3): ExecutorLostFailure (executor 3 exited caused by one of the running tasks) Reason: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.\n",
      "25/09/12 08:53:44 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_68_13 !\n",
      "25/09/12 08:53:44 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_57_14 !\n",
      "25/09/12 08:53:44 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_57_10 !\n",
      "25/09/12 08:53:44 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_57_6 !\n",
      "25/09/12 08:53:44 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_57_2 !\n",
      "[Stage 40:=>               (1 + 0) / 16][Stage 42:====>           (4 + -1) / 16]\r"
     ]
    }
   ],
   "source": [
    "# stopping the cluster\n",
    "! $SPARK_HOME/sbin/stop-all.sh"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
