{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de482d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# general imports\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import numpy.typing as npt\n",
    "import os\n",
    "import pandas as pd\n",
    "import time\n",
    "import warnings, logging\n",
    "\n",
    "from functools import singledispatch\n",
    "\n",
    "# dataset\n",
    "from sklearn.datasets import fetch_kddcup99\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# pyspark module\n",
    "from pyspark.rdd import RDD\n",
    "\n",
    "# src module\n",
    "from src.utils import kddSetup, sparkSetup\n",
    "from src.kmeans.base import compute_cost\n",
    "from src.kmeans.initialization import kMeansParallel_init, kMeansPlusPlus_init, kMeansRandom_init\n",
    "from src.kmeans.update import lloydKMeans, miniBatchKMeans"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afad672d",
   "metadata": {},
   "source": [
    "Starting the cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9fc77437",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the zipped environment if it doesn't already exist\n",
    "! if [ ! -f \"environment.tar.gz\" ]; then venv-pack -o \"environment.tar.gz\" ; fi\n",
    "# creating the zipped module src\n",
    "! if [ -f \"src.tar.gz\" ]; then rm src.tar.gz ; fi\n",
    "! tar -czf src.tar.gz src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cc6dba2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting org.apache.spark.deploy.master.Master, logging to /usr/local/spark/logs/spark-ubuntu-org.apache.spark.deploy.master.Master-1-mapd-b-14-1.out\n",
      "master: starting org.apache.spark.deploy.worker.Worker, logging to /usr/local/spark/logs/spark-ubuntu-org.apache.spark.deploy.worker.Worker-1-mapd-b-14-1.out\n",
      "worker1: starting org.apache.spark.deploy.worker.Worker, logging to /usr/local/spark/logs/spark-ubuntu-org.apache.spark.deploy.worker.Worker-1-mapd-b-14-2.out\n",
      "worker2: starting org.apache.spark.deploy.worker.Worker, logging to /usr/local/spark/logs/spark-ubuntu-org.apache.spark.deploy.worker.Worker-1-mapd-b-14-3.out\n",
      "worker3: starting org.apache.spark.deploy.worker.Worker, logging to /usr/local/spark/logs/spark-ubuntu-org.apache.spark.deploy.worker.Worker-1-mapd-b-14-4.out\n"
     ]
    }
   ],
   "source": [
    "# starting the cluster\n",
    "! $SPARK_HOME/sbin/start-all.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c8d733ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"PYSPARK_PYTHON\"] = \"./environment/bin/python\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3a075367",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/09/12 19:55:43 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/09/12 19:55:53 WARN Utils: Untarring behavior will be deprecated at spark.files and SparkContext.addFile. Consider using spark.archives or SparkContext.addArchive instead.\n"
     ]
    }
   ],
   "source": [
    "# creating a sparkSession\n",
    "spark = sparkSetup(\"kMeans\")\n",
    "sc = spark.sparkContext\n",
    "# exporting the src module\n",
    "sc.addPyFile(\"src.tar.gz\")\n",
    "sc.setLogLevel(\"ERROR\")\n",
    "# Setup the spark warnings\n",
    "log4j_conf_path = \"./Settings//log4j.properties\"\n",
    "warnings.filterwarnings(\"ignore\") \n",
    "logging.getLogger('py4j').setLevel(logging.ERROR) \n",
    "logging.getLogger('pyspark').setLevel(logging.ERROR) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e086be1b",
   "metadata": {},
   "source": [
    "Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3e68db5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "kdd_data, kdd_labels, entries_dict = kddSetup(standardize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9bb0b658",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = np.unique(kdd_labels).shape[0]\n",
    "l = 0.5 * k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4493579f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ParallelCollectionRDD[0] at readRDDFromFile at PythonRDD.scala:289"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_rdd = sc.parallelize([row for row in kdd_data]).persist()\n",
    "centroids = kMeansParallel_init(data_rdd, k, l)\n",
    "data_rdd.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5ff3e80d",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_dict = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b73df2",
   "metadata": {},
   "source": [
    "Lloyd's kMeans serial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2cfa59fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "final_centroids = lloydKMeans(kdd_data, centroids, epochs = 10)\n",
    "delta_time = time.time() - start_time\n",
    "time_dict[1] = {\"lloyd\": delta_time}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db70758a",
   "metadata": {},
   "source": [
    "Parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "be94e0a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONVERGED! in 4 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONVERGED! in 5 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONVERGED! in 5 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONVERGED! in 7 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONVERGED! in 4 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONVERGED! in 4 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "partitions = [2, 4, 8, 16, 32, 64, 128]\n",
    "for numSlices in partitions:\n",
    "    time_dict[numSlices] = {}\n",
    "    data_rdd = sc.parallelize([row for row in kdd_data], numSlices=numSlices).persist()\n",
    "    \n",
    "    start_time = time.time()\n",
    "    final_centroids = lloydKMeans(data_rdd, centroids, epochs = 10)\n",
    "    delta_time = time.time() - start_time\n",
    "    time_dict[numSlices][\"lloyd\"] = delta_time\n",
    "\n",
    "    start_time = time.time()\n",
    "    final_centroids = miniBatchKMeans(data_rdd, centroids, epochs = 10)\n",
    "    delta_time = time.time() - start_time\n",
    "    time_dict[numSlices][\"miniBatch\"] = delta_time\n",
    "    \n",
    "    data_rdd.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "018ba462",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: {'lloyd': 26.646967887878418},\n",
       " 2: {'lloyd': 111.75289154052734, 'miniBatch': 14.334075689315796},\n",
       " 4: {'lloyd': 63.27473545074463, 'miniBatch': 14.519975185394287},\n",
       " 8: {'lloyd': 39.25138187408447, 'miniBatch': 25.16306710243225},\n",
       " 16: {'lloyd': 31.23866581916809, 'miniBatch': 8.106281757354736},\n",
       " 32: {'lloyd': 32.67539715766907, 'miniBatch': 15.810601234436035},\n",
       " 64: {'lloyd': 38.274736642837524, 'miniBatch': 7.483483076095581},\n",
       " 128: {'lloyd': 52.767820835113525, 'miniBatch': 10.829732656478882}}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ba66583",
   "metadata": {},
   "source": [
    "### Cost vs iterations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fde6a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_epochs = 200\n",
    "\n",
    "# final results dictionary\n",
    "results_dict = {\n",
    "    \"lloyd_serial\": [],\n",
    "    \"lloyd_parallel\": [],\n",
    "    \"minibatch_01\": [],\n",
    "    \"minibatch_02\": [],\n",
    "    \"minibatch_05\": [],\n",
    "    \"minibatch_07\": []\n",
    "}\n",
    "\n",
    "# partial_results\n",
    "patience = 3       # number of previous costs to average for stopping\n",
    "threshold = 1e-3    # convergence threshold\n",
    "\n",
    "# Parameters\n",
    "n_runs = 3  # number of independent runs\n",
    "all_histories_runs = {key: [] for key in results_dict.keys()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5abc045c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in results_dict.keys():\n",
    "    print(f\"Running {key}\")\n",
    "\n",
    "    for run in range(n_runs):\n",
    "        print(f\"  Run {run+1}/{n_runs}\")\n",
    "        history = []\n",
    "\n",
    "        current_centroids = centroids.copy()  # initialize centroids\n",
    "\n",
    "        # compute initial cost\n",
    "        if key.startswith(\"minibatch\") or key == \"lloyd_parallel\":\n",
    "            initial_cost = compute_cost(data_rdd, current_centroids)\n",
    "        else:  # lloyd_serial\n",
    "            initial_cost = compute_cost(kdd_data, current_centroids)\n",
    "        history.append(initial_cost)\n",
    "\n",
    "        # compute costs for each epoch\n",
    "        for epoch in range(max_epochs):\n",
    "            if key == 'lloyd_serial':\n",
    "                current_centroids = lloydKMeans(kdd_data, centroids=current_centroids, epochs=1)\n",
    "                cost = compute_cost(kdd_data, current_centroids)\n",
    "            elif key == \"lloyd_parallel\":\n",
    "                current_centroids = lloydKMeans(data_rdd, centroids=current_centroids, epochs=1)\n",
    "                cost = compute_cost(data_rdd, current_centroids)\n",
    "            elif key.startswith(\"minibatch\"):\n",
    "                fraction = {\n",
    "                    \"minibatch_01\": 0.1,\n",
    "                    \"minibatch_02\": 0.2,\n",
    "                    \"minibatch_05\": 0.5,\n",
    "                    \"minibatch_07\": 0.7,\n",
    "                }[key]\n",
    "\n",
    "                current_centroids = miniBatchKMeans(\n",
    "                    data_rdd, centroids=current_centroids, epochs=1, batch_fraction=fraction\n",
    "                )\n",
    "                cost = compute_cost(data_rdd, current_centroids)\n",
    "\n",
    "            history.append(cost)\n",
    "\n",
    "            # early stopping: compare current cost to average of last `patience` costs\n",
    "            if len(history) > patience:\n",
    "                recent_avg = np.mean(history[-patience:])\n",
    "                if abs(history[-1] - recent_avg) < threshold:\n",
    "                    break\n",
    "\n",
    "        # pad history to max_epochs\n",
    "        if len(history) < max_epochs + 1:  # +1 because of initial cost\n",
    "            history += [history[-1]] * ((max_epochs + 1) - len(history))\n",
    "\n",
    "        all_histories_runs[key].append(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "076a5a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute mean and std for each algorithm\n",
    "results_mean_std = {}\n",
    "for key, histories in all_histories_runs.items():\n",
    "    arr = np.array(histories)  # shape: (n_runs, max_epochs+1)\n",
    "    mean_cost = arr.mean(axis=0)\n",
    "    std_cost = arr.std(axis=0)\n",
    "    results_mean_std[key] = {\"mean\": mean_cost, \"std\": std_cost}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28dc5b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# different markers\n",
    "markers = ['o', 's', '^', 'D', 'x', '*']\n",
    "\n",
    "offsets = [0,1,0,0,1,-1]\n",
    "for i, (key, stats) in enumerate(results_mean_std.items()):\n",
    "    mean = stats[\"mean\"]\n",
    "    std = stats[\"std\"]\n",
    "    epochs = np.arange(len(mean))\n",
    "\n",
    "    # style\n",
    "    if \"lloyd\" in key:\n",
    "        line_style = \"--\"   \n",
    "    else:\n",
    "        line_style = \"-\"    \n",
    "\n",
    "    # cycle markers\n",
    "    marker = markers[i]\n",
    "\n",
    "    #offset to avoid errorbar overlaps\n",
    "    \n",
    "    plt.errorbar(\n",
    "        epochs[::10+offsets[i]], mean[::10+offsets[i]], yerr=std[::10+offsets[i]],\n",
    "        fmt=line_style+marker, capsize=3, label=key\n",
    "    )\n",
    "    offset += 1\n",
    "\n",
    "plt.xlabel(\"Epoch\", fontsize=14)\n",
    "plt.ylabel(\"Cost\", fontsize=14)\n",
    "plt.title(\"K-Means Convergence: Cost vs Epochs\", fontsize=16)\n",
    "plt.grid(alpha=0.6)\n",
    "plt.legend(title=\"Algorithm:\", loc=\"best\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa6ebe5b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "291a2313",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eb2904b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cacc13c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ccb4ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99094631",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60c15790",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "82c23915",
   "metadata": {},
   "source": [
    "Stopping the cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "501de77c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8ec63757",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "worker1: stopping org.apache.spark.deploy.worker.Worker\n",
      "worker3: stopping org.apache.spark.deploy.worker.Worker\n",
      "worker2: stopping org.apache.spark.deploy.worker.Worker\n",
      "master: stopping org.apache.spark.deploy.worker.Worker\n",
      "stopping org.apache.spark.deploy.master.Master\n"
     ]
    }
   ],
   "source": [
    "# stopping the cluster\n",
    "! $SPARK_HOME/sbin/stop-all.sh\n",
    "# clearing the `$SPARK_HOME/work` directory in all the nodes\n",
    "# this avoids cluttering of storage among nodes\n",
    "! rm -rf $SPARK_HOME/work/*\n",
    "! ssh worker1 \"rm -rf $SPARK_HOME/work/*\"\n",
    "! ssh worker2 \"rm -rf $SPARK_HOME/work/*\"\n",
    "! ssh worker3 \"rm -rf $SPARK_HOME/work/*\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69cc7d6c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
