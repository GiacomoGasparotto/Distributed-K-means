{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cb95c923",
   "metadata": {},
   "outputs": [],
   "source": [
    "# general imports\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import numpy.typing as npt\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# dataset\n",
    "from sklearn.datasets import fetch_kddcup99\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# pyspark module\n",
    "from pyspark.rdd import RDD\n",
    "\n",
    "# src module\n",
    "from src.utils import sparkSetup\n",
    "from src.kmeans import compute_centroidDistances, get_clusterId, get_minDistance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "08e6fa38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the zipped environment if it doesn't already exist\n",
    "! if [ ! -f \"environment.tar.gz\" ]; then venv-pack -o \"environment.tar.gz\" ; fi\n",
    "# creating the zipped module src\n",
    "! if [ -f \"src.tar.gz\" ]; then rm src.tar.gz ; fi\n",
    "! tar -czf src.tar.gz src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3bf3e878",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting org.apache.spark.deploy.master.Master, logging to /usr/local/spark/logs/spark-ubuntu-org.apache.spark.deploy.master.Master-1-mapd-b-14-1.out\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "master: starting org.apache.spark.deploy.worker.Worker, logging to /usr/local/spark/logs/spark-ubuntu-org.apache.spark.deploy.worker.Worker-1-mapd-b-14-1.out\n",
      "worker3: starting org.apache.spark.deploy.worker.Worker, logging to /usr/local/spark/logs/spark-ubuntu-org.apache.spark.deploy.worker.Worker-1-mapd-b-14-4.out\n",
      "worker2: starting org.apache.spark.deploy.worker.Worker, logging to /usr/local/spark/logs/spark-ubuntu-org.apache.spark.deploy.worker.Worker-1-mapd-b-14-3.out\n",
      "worker1: starting org.apache.spark.deploy.worker.Worker, logging to /usr/local/spark/logs/spark-ubuntu-org.apache.spark.deploy.worker.Worker-1-mapd-b-14-2.out\n"
     ]
    }
   ],
   "source": [
    "# starting the cluster\n",
    "! $SPARK_HOME/sbin/start-all.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f5f8c42b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# telling spark where to find the python binary\n",
    "os.environ[\"PYSPARK_PYTHON\"] = \"./environment/bin/python\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "56bd4ed6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/09/09 19:37:29 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/09/09 19:37:35 WARN Utils: Untarring behavior will be deprecated at spark.files and SparkContext.addFile. Consider using spark.archives or SparkContext.addArchive instead.\n"
     ]
    }
   ],
   "source": [
    "# creating a sparkSession\n",
    "spark = sparkSetup(\"kMeans\")\n",
    "sc = spark.sparkContext\n",
    "# exporting the src module\n",
    "sc.addPyFile(\"src.tar.gz\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05cf1762",
   "metadata": {},
   "source": [
    "### Naming conventions\n",
    "\n",
    "The single datum is named as `datumName`, while the RDD that is a collection of one or more data is called `datumName_rdd`.\n",
    "\n",
    "Example: `compute_clusterDistances` returns `clusterDistances` (i.e. a numpy array of distances between a point `x` and the `centroids`). \n",
    "The RDD that collects all the `clusterDistances` will be called `clusterDistances_rdd`, and here is a sample implementation of that:\n",
    "```python\n",
    "def compute_centroidDistances(x, centroids):\n",
    "    return np.sum((centroids - x)**2, axis = 1)\n",
    "\n",
    "# `data_rdd` is an RDD\n",
    "# `centroids` is a numpy array\n",
    "clusterDistances_rdd = data_rdd \\\n",
    "    .map(lambda x: compute_clusterDistances(x, centroids))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5a86e34",
   "metadata": {},
   "source": [
    "### Load and preprocess the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a245d743",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fetch the dataset and its labels\n",
    "kdd_data, kdd_labels = fetch_kddcup99(\n",
    "    percent10 = True,\n",
    "    shuffle = True,\n",
    "    return_X_y = True\n",
    ")\n",
    "# transform bytes entries into integers\n",
    "entries_dict = {\n",
    "    i: np.unique(kdd_data[:,i], return_inverse=True) \n",
    "        for i in range(kdd_data.shape[1]) \n",
    "            if isinstance(kdd_data[0,i], bytes) \n",
    "}\n",
    "for key, values in entries_dict.items():\n",
    "    kdd_data[:,key] = values[1]\n",
    "# and then cast everything into a float\n",
    "kdd_data = kdd_data.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9e8124d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Real dataset\n",
    "\n",
    "# Change percent10 to 'False' to fetch the full dataset (4M rows)\n",
    "# for local works is better to leave it as 'True'\n",
    "kdd = fetch_kddcup99(shuffle=True,percent10=True) \n",
    "kdd_data = kdd.data\n",
    "\n",
    "# Remove string features and standardize them\n",
    "data = np.delete(kdd_data,np.arange(1,4,1),axis = 1) \n",
    "scaler = StandardScaler()\n",
    "data = scaler.fit_transform(data)\n",
    "\n",
    "#parallelize\n",
    "data_rdd = sc.parallelize([row for row in data])\n",
    "data_rdd = data_rdd.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4622aac9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "UniqueCountsResult(values=array([b'back.', b'buffer_overflow.', b'ftp_write.', b'guess_passwd.',\n",
       "       b'imap.', b'ipsweep.', b'land.', b'loadmodule.', b'multihop.',\n",
       "       b'neptune.', b'nmap.', b'normal.', b'perl.', b'phf.', b'pod.',\n",
       "       b'portsweep.', b'rootkit.', b'satan.', b'smurf.', b'spy.',\n",
       "       b'teardrop.', b'warezclient.', b'warezmaster.'], dtype=object), counts=array([  2203,     30,      8,     53,     12,   1247,     21,      9,\n",
       "            7, 107201,    231,  97278,      3,      4,    264,   1040,\n",
       "           10,   1589, 280790,      2,    979,   1020,     20]))"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique_counts(kdd.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "c8077baf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "UniqueCountsResult(values=array([b'IRC', b'X11', b'Z39_50', b'auth', b'bgp', b'courier',\n",
       "       b'csnet_ns', b'ctf', b'daytime', b'discard', b'domain',\n",
       "       b'domain_u', b'echo', b'eco_i', b'ecr_i', b'efs', b'exec',\n",
       "       b'finger', b'ftp', b'ftp_data', b'gopher', b'hostnames', b'http',\n",
       "       b'http_443', b'imap4', b'iso_tsap', b'klogin', b'kshell', b'ldap',\n",
       "       b'link', b'login', b'mtp', b'name', b'netbios_dgm', b'netbios_ns',\n",
       "       b'netbios_ssn', b'netstat', b'nnsp', b'nntp', b'ntp_u', b'other',\n",
       "       b'pm_dump', b'pop_2', b'pop_3', b'printer', b'private', b'red_i',\n",
       "       b'remote_job', b'rje', b'shell', b'smtp', b'sql_net', b'ssh',\n",
       "       b'sunrpc', b'supdup', b'systat', b'telnet', b'tftp_u', b'tim_i',\n",
       "       b'time', b'urh_i', b'urp_i', b'uucp', b'uucp_path', b'vmnet',\n",
       "       b'whois'], dtype=object), counts=array([    43,     11,     92,    328,    106,    108,    126,     97,\n",
       "          103,    116,    116,   5863,    112,   1642, 281400,    103,\n",
       "           99,    670,    798,   4721,    117,    104,  64293,     99,\n",
       "          117,    115,    106,     98,    101,    102,    104,    107,\n",
       "           98,     99,    102,    107,     95,    105,    108,    380,\n",
       "         7237,      1,    101,    202,    109, 110893,      1,    120,\n",
       "          111,    112,   9723,    110,    105,    107,    105,    115,\n",
       "          513,      1,      7,    157,     14,    538,    106,    106,\n",
       "          106,    110]))"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique_counts(kdd_data[:,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "62771d9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, b'icmp', b'ecr_i', b'SF', 1032, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 511, 511, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 255,\n",
       "       255, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0], dtype=object)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kdd_data[1,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "2cb44751",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(141480, 38)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(data, axis = 0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c573e18d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(494021, 38)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5d395c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.random.rand(100, 2)\n",
    "data_rdd = sc.parallelize([row for row in data])\n",
    "data_rdd = data_rdd.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f5c72d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kMeansRandom_init(\n",
    "    data_rdd: RDD,\n",
    "    k: int\n",
    ") -> npt.NDArray:\n",
    "    centroids = np.array(\n",
    "        data_rdd.takeSample(withReplacement=False, num=k)\n",
    "    )\n",
    "    return centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f4edab4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kMeansPlusPlus_init(\n",
    "    data: npt.NDArray,\n",
    "    k: int,\n",
    "    weights: npt.NDArray = np.array([])\n",
    ") -> npt.NDArray:\n",
    "    \"\"\"\n",
    "    Standard kMeans++ initialization method:\n",
    "    given `data` (eventually weighted), returns `k` cluster centroids\n",
    "    \"\"\"\n",
    "    if weights.shape[0] == 0:\n",
    "        weights = np.ones(shape=(data.shape[0],1))\n",
    "    \n",
    "    centroids = data[np.random.randint(0, data.shape[0]),:].reshape(1, -1) # reshaping for easier stacking\n",
    "    \n",
    "    while (centroids.shape[0] < k):\n",
    "        # since the original functions are made for map\n",
    "        # we need to loop over the data\n",
    "        minDistance_array = np.array(\n",
    "            [get_minDistance(compute_centroidDistances(datum, centroids)) for datum in data]\n",
    "        ) * weights # multiplyling by the weight simulates multiple copies of the same datum\n",
    "        total_minDistance = np.sum(minDistance_array)\n",
    "        # sampling probability proportional to minDistance\n",
    "        new_centroid_idx = np.random.choice(minDistance_array.shape[0], size = 1, p = minDistance_array / total_minDistance)\n",
    "        new_centroid = data[new_centroid_idx,:].reshape(1, -1)\n",
    "\n",
    "        # edge case in which the same centroid is selected twice:\n",
    "        # redo the iteration without saving the centroid\n",
    "        if any(np.array_equal(new_centroid, row) for row in centroids): continue\n",
    "        centroids = np.concatenate((centroids, new_centroid), axis = 0)\n",
    "\n",
    "    return centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "49a63d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kMeansNaive(\n",
    "    data: npt.NDArray,\n",
    "    centroids: npt.NDArray,\n",
    "    epochs: int = 5\n",
    ") -> npt.NDArray:\n",
    "    \"\"\"\n",
    "    Standard kMeans algorithm:\n",
    "    given `data`, updates the (k) `centroids` for `epochs` times,\n",
    "    improving the clustering each time\n",
    "    \"\"\"\n",
    "    k = centroids.shape[0]\n",
    "    for _ in range(epochs):\n",
    "        assignments = np.array(\n",
    "            [get_clusterId(compute_centroidDistances(x, centroids)) for x in data]\n",
    "        )\n",
    "        centroids = np.array(\n",
    "            [np.mean(data[assignments==i,:], axis = 0) for i in range(k)]\n",
    "        )\n",
    "    return centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bb40ef7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kMeansParallel_init(\n",
    "    data_rdd: RDD,\n",
    "    k: int,\n",
    "    l: float\n",
    ") -> npt.NDArray:\n",
    "    \"\"\"\n",
    "    kMeans|| initialization method:\n",
    "    returns `k` good `centroids`.\n",
    "    `l` controls the probability of each point\n",
    "    in `data_rdd` of being sampled as a pre-processed centroid.\n",
    "    \"\"\"\n",
    "\n",
    "    centroids = np.array(\n",
    "        data_rdd.takeSample(num=1, withReplacement=False)\n",
    "    )\n",
    "    \n",
    "    minDistance_rdd = data_rdd \\\n",
    "        .map(lambda x: (x, get_minDistance(compute_centroidDistances(x, centroids)))) \\\n",
    "        .persist()\n",
    "\n",
    "    cost = minDistance_rdd \\\n",
    "        .map(lambda x: x[1]) \\\n",
    "        .sum()\n",
    "\n",
    "    iterations = int(np.ceil(np.log(cost))) if (cost > 1) else 1\n",
    "    for _ in range(iterations):\n",
    "        new_centroids = np.array(\n",
    "            minDistance_rdd \\\n",
    "                .filter(lambda x: np.random.rand() < np.min((l * x[1] / cost, 1))) \\\n",
    "                .map(lambda x: x[0]) \\\n",
    "                .collect()\n",
    "        )\n",
    "        # edge case in which no new centroid is sampled:\n",
    "        # this avoids the following `np.concatenate` to fail\n",
    "        if len(new_centroids.shape) < 2:\n",
    "            continue\n",
    "\n",
    "        minDistance_rdd.unpersist()\n",
    "        centroids = np.unique(\n",
    "            np.concatenate((centroids, new_centroids), axis = 0), \n",
    "            axis = 0\n",
    "        )\n",
    "\n",
    "        minDistance_rdd = data_rdd \\\n",
    "            .map(lambda x: (x, get_minDistance(compute_centroidDistances(x, centroids)))) \\\n",
    "            .persist()\n",
    "        cost = minDistance_rdd \\\n",
    "            .map(lambda x: x[1]) \\\n",
    "            .sum()\n",
    "    \n",
    "    minDistance_rdd.unpersist()\n",
    "    clusterCounts = data_rdd \\\n",
    "        .map(lambda x: (get_clusterId(compute_centroidDistances(x, centroids)), 1)) \\\n",
    "        .countByKey()\n",
    "    \n",
    "    clusterCounts = np.array([w[1] for w in clusterCounts.items()])\n",
    "    centroids = kMeansNaive(\n",
    "        centroids, \n",
    "        kMeansPlusPlus_init(centroids, k, clusterCounts)\n",
    "    )\n",
    "    \n",
    "    return centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "119cdd71",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/09/09 08:28:44 WARN TaskSetManager: Stage 24 contains a task of very large size (10124 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/09/09 08:28:45 WARN TaskSetManager: Stage 25 contains a task of very large size (10124 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "k = 15\n",
    "l = k * 10\n",
    "centroids = kMeansParallel_init(data_rdd, k, l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "624ac23f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 38)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(centroids, axis = 0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "07e96995",
   "metadata": {},
   "outputs": [],
   "source": [
    "def miniBatchKMeans(\n",
    "    data_rdd: RDD,\n",
    "    centroids: npt.NDArray,\n",
    "    iterations: int = 10,\n",
    "    batch_fraction: float = 0.1\n",
    ") -> npt.NDArray:\n",
    "    k = centroids.shape[0]\n",
    "    clusterCounters = np.zeros((k,)) # 1 / learning_rate\n",
    "    for iter in range(iterations):\n",
    "        miniBatch_rdd = data_rdd \\\n",
    "            .sample(withReplacement=False, fraction=batch_fraction)\n",
    "        miniBatch_rdd = miniBatch_rdd \\\n",
    "            .map(lambda x: (get_clusterId(compute_centroidDistances(x, centroids)), 1, x)) \\\n",
    "            .persist()\n",
    "        \n",
    "        # counting how many assigments per cluster\n",
    "        clusterCounts_dict = miniBatch_rdd \\\n",
    "            .map(lambda x: (x[0], x[1])) \\\n",
    "            .countByKey()\n",
    "        clusterCounts = np.array(\n",
    "            [clusterCounts_dict[i] if i in clusterCounts_dict.keys() else 0 for i in range(k)]\n",
    "        )\n",
    "        clusterCounters += clusterCounts\n",
    "        \n",
    "        # edge case in which a cluster has no assignments:\n",
    "        # if also its counter is zero the whole iteration is repeated\n",
    "        if any(np.isclose(v, 0) for v in clusterCounters): \n",
    "            iter -= 1\n",
    "            miniBatch_rdd.unpersist()\n",
    "            continue\n",
    "        # otherwise its count will be set to 1 to avoid division by 0 in the update step\n",
    "        clusterCounts = np.where(clusterCounts >= 1, clusterCounts, 1)\n",
    "\n",
    "        # summing all points assigned to the same cluster\n",
    "        # (in the update step this will be divided by the counts \n",
    "        # in order to get the mean for every cluster).\n",
    "        # A dict is used for convenience and consistency with clusterCounts\n",
    "        clusterSums_dict = dict(miniBatch_rdd \\\n",
    "            .map(lambda x: (x[0], x[2])) \\\n",
    "            .reduceByKey(lambda x, y: x + y) \\\n",
    "            .collect()\n",
    "        )\n",
    "        # edge case in which a cluster has no assignments:\n",
    "        # the centroid is returned instead of 0 \n",
    "        # (which would have been the sum of its assigned points) \n",
    "        # in order to not update its position \n",
    "        # (note how the terms cancel out in the update step)\n",
    "        clusterSums = np.array(\n",
    "            [clusterSums_dict[i] if i in clusterSums_dict.keys() else centroids[i,:] for i in range(k)]\n",
    "        )\n",
    "\n",
    "        # update step: c <- (1 - eta) * c + eta * x_mean\n",
    "        # (note x_mean = x_sums / c_count)\n",
    "        centroids = (1 - 1 / clusterCounters).reshape(-1, 1) * centroids + \\\n",
    "                    (1 / (clusterCounters * clusterCounts)).reshape(-1, 1) * clusterSums\n",
    "        \n",
    "        miniBatch_rdd.unpersist()\n",
    "        \n",
    "    return centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "30762a61",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/09/08 08:28:23 WARN TaskSetManager: Stage 40 contains a task of very large size (10124 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/09/08 08:28:24 WARN TaskSetManager: Stage 41 contains a task of very large size (10124 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/09/08 08:28:24 WARN TaskSetManager: Stage 42 contains a task of very large size (10124 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/09/08 08:28:25 WARN TaskSetManager: Stage 43 contains a task of very large size (10124 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/09/08 08:28:26 WARN TaskSetManager: Stage 44 contains a task of very large size (10124 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/09/08 08:28:27 WARN TaskSetManager: Stage 45 contains a task of very large size (10124 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/09/08 08:28:28 WARN TaskSetManager: Stage 46 contains a task of very large size (10124 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/09/08 08:28:29 WARN TaskSetManager: Stage 47 contains a task of very large size (10124 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/09/08 08:28:30 WARN TaskSetManager: Stage 48 contains a task of very large size (10124 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/09/08 08:28:30 WARN TaskSetManager: Stage 49 contains a task of very large size (10124 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "final_centroids = miniBatchKMeans(data_rdd, centroids, 10, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2abe2558",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f852301",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "worker2: stopping org.apache.spark.deploy.worker.Worker\n",
      "worker3: stopping org.apache.spark.deploy.worker.Worker\n",
      "worker1: stopping org.apache.spark.deploy.worker.Worker\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/09/09 20:16:13 ERROR TaskSchedulerImpl: Lost executor 2 on 10.67.22.202: Command exited with code 143\n",
      "25/09/09 20:16:13 ERROR TaskSchedulerImpl: Lost executor 0 on 10.67.22.208: Worker shutting down\n",
      "25/09/09 20:16:13 ERROR TaskSchedulerImpl: Lost executor 3 on 10.67.22.170: Command exited with code 143\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "master: stopping org.apache.spark.deploy.worker.Worker\n",
      "stopping org.apache.spark.deploy.master.Master\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/09/09 20:16:14 WARN StandaloneAppClient$ClientEndpoint: Connection to 10.67.22.224:7077 failed; waiting for master to reconnect...\n",
      "25/09/09 20:16:14 WARN StandaloneSchedulerBackend: Disconnected from Spark cluster! Waiting for reconnection...\n"
     ]
    }
   ],
   "source": [
    "# stopping the cluster\n",
    "! $SPARK_HOME/sbin/stop-all.sh"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
