{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb95c923",
   "metadata": {},
   "outputs": [],
   "source": [
    "# general imports\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import numpy.typing as npt\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# dataset\n",
    "from sklearn.datasets import fetch_kddcup99\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# pyspark module\n",
    "from pyspark.rdd import RDD\n",
    "\n",
    "# src module\n",
    "from src.utils import sparkSetup\n",
    "from src.kmeans import compute_centroidDistances, get_clusterId, get_minDistance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "08e6fa38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the zipped environment if it doesn't already exist\n",
    "! if [ ! -f \"environment.tar.gz\" ]; then venv-pack -o \"environment.tar.gz\" ; fi\n",
    "# creating the zipped module src\n",
    "! if [ -f \"src.tar.gz\" ]; then rm src.tar.gz ; fi\n",
    "! tar -czf src.tar.gz src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3bf3e878",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "org.apache.spark.deploy.master.Master running as process 83783.  Stop it first.\n",
      "worker3: org.apache.spark.deploy.worker.Worker running as process 51120.  Stop it first.\n",
      "worker2: org.apache.spark.deploy.worker.Worker running as process 50820.  Stop it first.\n",
      "worker1: org.apache.spark.deploy.worker.Worker running as process 51494.  Stop it first.\n",
      "master: org.apache.spark.deploy.worker.Worker running as process 83946.  Stop it first.\n"
     ]
    }
   ],
   "source": [
    "# starting the cluster\n",
    "! $SPARK_HOME/sbin/start-all.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f8c42b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# telling spark where to find the python binary\n",
    "os.environ[\"PYSPARK_PYTHON\"] = \"./environment/bin/python\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56bd4ed6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/09/08 09:51:36 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/09/08 09:51:42 WARN Utils: Untarring behavior will be deprecated at spark.files and SparkContext.addFile. Consider using spark.archives or SparkContext.addArchive instead.\n"
     ]
    }
   ],
   "source": [
    "# creating a sparkSession\n",
    "spark = sparkSetup(\"kMeans\")\n",
    "sc = spark.sparkContext\n",
    "# exporting the src module\n",
    "sc.addPyFile(\"src.tar.gz\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05cf1762",
   "metadata": {},
   "source": [
    "### Naming conventions\n",
    "\n",
    "The single datum is named as `datumName`, while the RDD that is a collection of one or more data is called `datumName_rdd`.\n",
    "\n",
    "Example: `compute_clusterDistances` returns `clusterDistances` (i.e. a numpy array of distances between a point `x` and the `centroids`). \n",
    "The RDD that collects all the `clusterDistances` will be called `clusterDistances_rdd`, and here is a sample implementation of that:\n",
    "```python\n",
    "def compute_centroidDistances(x, centroids):\n",
    "    return np.sum((centroids - x)**2, axis = 1)\n",
    "\n",
    "# `data_rdd` is an RDD\n",
    "# `centroids` is a numpy array\n",
    "clusterDistances_rdd = data_rdd \\\n",
    "    .map(lambda x: compute_clusterDistances(x, centroids))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5a86e34",
   "metadata": {},
   "source": [
    "### Load and preprocess the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e8124d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Real dataset\n",
    "\n",
    "# Change percent10 to 'False' to fetch the full dataset (4M rows)\n",
    "# for local works is better to leave it as 'True'\n",
    "kdd = fetch_kddcup99(shuffle=True,percent10=True) \n",
    "kdd_data = kdd.data\n",
    "\n",
    "# Remove string features and standardize them\n",
    "data = np.delete(kdd_data,np.arange(1,4,1),axis = 1) \n",
    "scaler = StandardScaler()\n",
    "data = scaler.fit_transform(data)\n",
    "\n",
    "#parallelize\n",
    "data_rdd = sc.parallelize([row for row in data])\n",
    "data_rdd = data_rdd.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f4edab4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kMeansPlusPlus_init(\n",
    "    data: npt.NDArray,\n",
    "    k: int,\n",
    "    weights: npt.NDArray = np.array([])\n",
    ") -> npt.NDArray:\n",
    "    \"\"\"\n",
    "    Standard kMeans++ initialization method:\n",
    "    given `data` (eventually weighted), returns `k` cluster centroids\n",
    "    \"\"\"\n",
    "    if weights.shape[0] == 0:\n",
    "        weights = np.ones(shape=(data.shape[0],1))\n",
    "    \n",
    "    centroids = data[np.random.randint(0, data.shape[0]),:].reshape(1, -1) # reshaping for easier stacking\n",
    "    \n",
    "    while (centroids.shape[0] < k):\n",
    "        # since the original functions are made for map\n",
    "        # we need to loop over the data\n",
    "        minDistance_array = np.array(\n",
    "            [get_minDistance(compute_centroidDistances(datum, centroids)) for datum in data]\n",
    "        ) * weights # multiplyling by the weight simulates multiple copies of the same datum\n",
    "        total_minDistance = np.sum(minDistance_array)\n",
    "        # sampling probability proportional to minDistance\n",
    "        new_centroid_idx = np.random.choice(minDistance_array.shape[0], size = 1, p = minDistance_array / total_minDistance)\n",
    "        new_centroid = data[new_centroid_idx,:].reshape(1, -1)\n",
    "\n",
    "        # edge case in which the same centroid is selected twice:\n",
    "        # redo the iteration without saving the centroid\n",
    "        if any(np.array_equal(new_centroid, row) for row in centroids): continue\n",
    "        centroids = np.concatenate((centroids, new_centroid), axis = 0)\n",
    "\n",
    "    return centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "49a63d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kMeansNaive(\n",
    "    data: npt.NDArray,\n",
    "    centroids: npt.NDArray,\n",
    "    epochs: int = 5\n",
    ") -> npt.NDArray:\n",
    "    \"\"\"\n",
    "    Standard kMeans algorithm:\n",
    "    given `data`, updates the (k) `centroids` for `epochs` times,\n",
    "    improving the clustering each time\n",
    "    \"\"\"\n",
    "    k = centroids.shape[0]\n",
    "    for _ in range(epochs):\n",
    "        assignments = np.array(\n",
    "            [get_clusterId(compute_centroidDistances(x, centroids)) for x in data]\n",
    "        )\n",
    "        centroids = np.array(\n",
    "            [np.mean(data[assignments==i,:], axis = 0) for i in range(k)]\n",
    "        )\n",
    "    return centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bb40ef7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kMeansParallel_init(\n",
    "    data_rdd: RDD,\n",
    "    k: int,\n",
    "    l: float\n",
    ") -> npt.NDArray:\n",
    "    \"\"\"\n",
    "    kMeans|| initialization method:\n",
    "    returns `k` good `centroids`.\n",
    "    `l` controls the probability of each point\n",
    "    in `data_rdd` of being sampled as a pre-processed centroid.\n",
    "    \"\"\"\n",
    "\n",
    "    centroids = np.array(\n",
    "        data_rdd.takeSample(num=1, withReplacement=False)\n",
    "    )\n",
    "    \n",
    "    minDistance_rdd = data_rdd \\\n",
    "        .map(lambda x: (x, get_minDistance(compute_centroidDistances(x, centroids)))) \\\n",
    "        .persist()\n",
    "\n",
    "    cost = minDistance_rdd \\\n",
    "        .map(lambda x: x[1]) \\\n",
    "        .sum()\n",
    "\n",
    "    iterations = int(np.ceil(np.log(cost))) if (cost > 1) else 1\n",
    "    for _ in range(iterations):\n",
    "        new_centroids = np.array(\n",
    "            minDistance_rdd \\\n",
    "                .filter(lambda x: np.random.rand() < np.min((l * x[1] / cost, 1))) \\\n",
    "                .map(lambda x: x[0]) \\\n",
    "                .collect()\n",
    "        )\n",
    "        # edge case in which no new centroid is sampled:\n",
    "        # this avoids the following `np.concatenate` to fail\n",
    "        if len(new_centroids.shape) < 2:\n",
    "            continue\n",
    "\n",
    "        minDistance_rdd.unpersist()\n",
    "        centroids = np.unique(\n",
    "            np.concatenate((centroids, new_centroids), axis = 0), \n",
    "            axis = 0\n",
    "        )\n",
    "\n",
    "        minDistance_rdd = data_rdd \\\n",
    "            .map(lambda x: (x, get_minDistance(compute_centroidDistances(x, centroids)))) \\\n",
    "            .persist()\n",
    "        cost = minDistance_rdd \\\n",
    "            .map(lambda x: x[1]) \\\n",
    "            .sum()\n",
    "    \n",
    "    minDistance_rdd.unpersist()\n",
    "    clusterCounts = data_rdd \\\n",
    "        .map(lambda x: (get_clusterId(compute_centroidDistances(x, centroids)), 1)) \\\n",
    "        .countByKey()\n",
    "    \n",
    "    clusterCounts = np.array([w[1] for w in clusterCounts.items()])\n",
    "    centroids = kMeansNaive(\n",
    "        centroids, \n",
    "        kMeansPlusPlus_init(centroids, k, clusterCounts)\n",
    "    )\n",
    "    \n",
    "    return centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "119cdd71",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/09/08 09:52:19 WARN TaskSetManager: Stage 1 contains a task of very large size (10124 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/09/08 09:52:22 WARN TaskSetManager: Stage 2 contains a task of very large size (10124 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/09/08 09:52:23 WARN TaskSetManager: Stage 3 contains a task of very large size (10124 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/09/08 09:52:24 WARN TaskSetManager: Stage 4 contains a task of very large size (10124 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/09/08 09:52:25 WARN TaskSetManager: Stage 5 contains a task of very large size (10124 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/09/08 09:52:27 WARN TaskSetManager: Stage 6 contains a task of very large size (10124 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/09/08 09:52:28 WARN TaskSetManager: Stage 7 contains a task of very large size (10124 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/09/08 09:52:30 WARN TaskSetManager: Stage 8 contains a task of very large size (10124 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/09/08 09:52:31 WARN TaskSetManager: Stage 9 contains a task of very large size (10124 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/09/08 09:52:33 WARN TaskSetManager: Stage 10 contains a task of very large size (10124 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/09/08 09:52:34 WARN TaskSetManager: Stage 11 contains a task of very large size (10124 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/09/08 09:52:36 WARN TaskSetManager: Stage 12 contains a task of very large size (10124 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/09/08 09:52:37 WARN TaskSetManager: Stage 13 contains a task of very large size (10124 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/09/08 09:52:39 WARN TaskSetManager: Stage 14 contains a task of very large size (10124 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/09/08 09:52:40 WARN TaskSetManager: Stage 15 contains a task of very large size (10124 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/09/08 09:52:43 WARN TaskSetManager: Stage 16 contains a task of very large size (10124 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/09/08 09:52:44 WARN TaskSetManager: Stage 17 contains a task of very large size (10124 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/09/08 09:52:47 WARN TaskSetManager: Stage 18 contains a task of very large size (10124 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/09/08 09:52:47 WARN TaskSetManager: Stage 19 contains a task of very large size (10124 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/09/08 09:52:51 WARN TaskSetManager: Stage 20 contains a task of very large size (10124 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/09/08 09:52:51 WARN TaskSetManager: Stage 21 contains a task of very large size (10124 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/09/08 09:52:55 WARN TaskSetManager: Stage 22 contains a task of very large size (10124 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/09/08 09:52:56 WARN TaskSetManager: Stage 23 contains a task of very large size (10124 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/09/08 09:53:00 WARN TaskSetManager: Stage 24 contains a task of very large size (10124 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/09/08 09:53:00 WARN TaskSetManager: Stage 25 contains a task of very large size (10124 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/09/08 09:53:04 WARN TaskSetManager: Stage 26 contains a task of very large size (10124 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/09/08 09:53:05 WARN TaskSetManager: Stage 27 contains a task of very large size (10124 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/09/08 09:53:10 WARN TaskSetManager: Stage 28 contains a task of very large size (10124 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/09/08 09:53:11 WARN TaskSetManager: Stage 29 contains a task of very large size (10124 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/09/08 09:53:15 WARN TaskSetManager: Stage 30 contains a task of very large size (10124 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/09/08 09:53:16 WARN TaskSetManager: Stage 31 contains a task of very large size (10124 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/09/08 09:53:21 WARN TaskSetManager: Stage 32 contains a task of very large size (10124 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/09/08 09:53:22 WARN TaskSetManager: Stage 33 contains a task of very large size (10124 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/09/08 09:53:27 WARN TaskSetManager: Stage 34 contains a task of very large size (10124 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/09/08 09:53:28 WARN TaskSetManager: Stage 35 contains a task of very large size (10124 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/09/08 09:53:34 WARN TaskSetManager: Stage 36 contains a task of very large size (10124 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/09/08 09:53:34 WARN TaskSetManager: Stage 37 contains a task of very large size (10124 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/09/08 09:53:40 WARN TaskSetManager: Stage 38 contains a task of very large size (10124 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/09/08 10:00:19 ERROR TaskSchedulerImpl: Lost executor 2 on 10.67.22.208: Command exited with code 143\n",
      "25/09/08 10:00:19 ERROR TaskSchedulerImpl: Lost executor 0 on 10.67.22.170: Command exited with code 143\n",
      "25/09/08 10:00:19 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_0_11 !\n",
      "25/09/08 10:00:19 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_0_0 !\n",
      "25/09/08 10:00:19 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_0_7 !\n",
      "25/09/08 10:00:19 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_0_3 !\n",
      "25/09/08 10:00:19 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_0_9 !\n",
      "25/09/08 10:00:19 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_0_5 !\n",
      "25/09/08 10:00:19 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_0_1 !\n",
      "25/09/08 10:00:19 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_0_13 !\n",
      "25/09/08 10:00:19 ERROR TaskSchedulerImpl: Lost executor 1 on 10.67.22.202: Command exited with code 143\n",
      "25/09/08 10:00:19 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_0_2 !\n",
      "25/09/08 10:00:19 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_0_10 !\n",
      "25/09/08 10:00:19 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_0_14 !\n",
      "25/09/08 10:00:19 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_0_6 !\n",
      "25/09/08 10:00:20 WARN StandaloneAppClient$ClientEndpoint: Connection to 10.67.22.224:7077 failed; waiting for master to reconnect...\n",
      "25/09/08 10:00:20 WARN StandaloneSchedulerBackend: Disconnected from Spark cluster! Waiting for reconnection...\n",
      "25/09/08 10:00:25 ERROR TaskSchedulerImpl: Lost executor 3 on 10.67.22.224: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.\n",
      "25/09/08 10:00:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_0_12 !\n",
      "25/09/08 10:00:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_0_4 !\n",
      "25/09/08 10:00:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_0_15 !\n",
      "25/09/08 10:00:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_0_8 !\n"
     ]
    }
   ],
   "source": [
    "k = 10\n",
    "l = k * 10\n",
    "centroids = kMeansParallel_init(data_rdd, k, l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07e96995",
   "metadata": {},
   "outputs": [],
   "source": [
    "def miniBatchKMeans(\n",
    "    data_rdd: RDD,\n",
    "    centroids: npt.NDArray,\n",
    "    iterations: int = 10,\n",
    "    batch_fraction: float = 0.1\n",
    ") -> npt.NDArray:\n",
    "    k = centroids.shape[0] # number of centroids\n",
    "    clusterCounters = np.zeros((k,)) # 1 / learning_rate\n",
    "    for iter in range(iterations):\n",
    "        # sample uniformly from the partitions a batch_fraction of the total rdd dataset\n",
    "        miniBatch_rdd = data_rdd \\\n",
    "            .sample(withReplacement=False, fraction=batch_fraction)\n",
    "        # map procedure: assign each point to a cluster based on the distance with respect to the centroids and persist it \n",
    "        miniBatch_rdd = miniBatch_rdd \\\n",
    "            .map(lambda x: (get_clusterId(compute_centroidDistances(x, centroids)), 1, x)) \\\n",
    "            .persist()\n",
    "        \n",
    "        # reduce procedure: counting how many assigments per cluster\n",
    "        clusterCounts_dict = miniBatch_rdd \\\n",
    "            .map(lambda x: (x[0], x[1])) \\\n",
    "            .countByKey()\n",
    "        \n",
    "        # count how many points per cluster in this minibatch\n",
    "        clusterCounts = np.array(\n",
    "            [clusterCounts_dict[i] if i in clusterCounts_dict.keys() else 0 for i in range(k)]\n",
    "        )\n",
    "        # update the counts for the global counter\n",
    "        clusterCounters += clusterCounts\n",
    "        \n",
    "        # edge case in which a cluster has no assignments:\n",
    "        # if also its counter is zero the whole iteration is repeated\n",
    "        if any(np.isclose(v, 0) for v in clusterCounters): \n",
    "            iter -= 1\n",
    "            miniBatch_rdd.unpersist()\n",
    "            continue\n",
    "        # otherwise its count will be set to 1 to avoid division by 0 in the update step\n",
    "        clusterCounts = np.where(clusterCounts >= 1, clusterCounts, 1)\n",
    "\n",
    "        # summing all points assigned to the same cluster\n",
    "        # (in the update step this will be divided by the counts \n",
    "        # in order to get the mean for every cluster).\n",
    "        # reduce procedure: in this case a dict is used for convenience and consistency with clusterCounts \n",
    "        clusterSums_dict = dict(miniBatch_rdd \\\n",
    "            .map(lambda x: (x[0], x[2])) \\\n",
    "            .reduceByKey(lambda x, y: x + y) \\\n",
    "            .collect()\n",
    "        )\n",
    "        # edge case in which a cluster has no assignments:\n",
    "        # the centroid is returned instead of 0 \n",
    "        # (which would have been the sum of its assigned points) \n",
    "        # in order to not update its position \n",
    "        # (note how the terms cancel out in the update step)\n",
    "        clusterSums = np.array(\n",
    "            [clusterSums_dict[i] if i in clusterSums_dict.keys() else centroids[i,:] for i in range(k)]\n",
    "        )\n",
    "\n",
    "        # update step: c <- (1 - eta) * c + eta * x_mean\n",
    "        # (note x_mean = x_sums / c_count)\n",
    "        centroids = (1 - 1 / clusterCounters).reshape(-1, 1) * centroids + \\\n",
    "                    (1 / (clusterCounters * clusterCounts)).reshape(-1, 1) * clusterSums\n",
    "        \n",
    "        # free the memory \n",
    "        miniBatch_rdd.unpersist()\n",
    "        \n",
    "    return centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "30762a61",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/09/08 08:28:23 WARN TaskSetManager: Stage 40 contains a task of very large size (10124 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/09/08 08:28:24 WARN TaskSetManager: Stage 41 contains a task of very large size (10124 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/09/08 08:28:24 WARN TaskSetManager: Stage 42 contains a task of very large size (10124 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/09/08 08:28:25 WARN TaskSetManager: Stage 43 contains a task of very large size (10124 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/09/08 08:28:26 WARN TaskSetManager: Stage 44 contains a task of very large size (10124 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/09/08 08:28:27 WARN TaskSetManager: Stage 45 contains a task of very large size (10124 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/09/08 08:28:28 WARN TaskSetManager: Stage 46 contains a task of very large size (10124 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/09/08 08:28:29 WARN TaskSetManager: Stage 47 contains a task of very large size (10124 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/09/08 08:28:30 WARN TaskSetManager: Stage 48 contains a task of very large size (10124 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/09/08 08:28:30 WARN TaskSetManager: Stage 49 contains a task of very large size (10124 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "final_centroids = miniBatchKMeans(data_rdd, centroids, 10, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2abe2558",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6f852301",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "worker3: no org.apache.spark.deploy.worker.Worker to stop\n",
      "master: no org.apache.spark.deploy.worker.Worker to stop\n",
      "worker1: no org.apache.spark.deploy.worker.Worker to stop\n",
      "worker2: no org.apache.spark.deploy.worker.Worker to stop\n",
      "no org.apache.spark.deploy.master.Master to stop\n"
     ]
    }
   ],
   "source": [
    "# stopping the cluster\n",
    "! $SPARK_HOME/sbin/stop-all.sh"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
