{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cb95c923",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import numpy.typing as npt\n",
    "import pandas as pd\n",
    "from sklearn.datasets import fetch_kddcup99\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.rdd import RDD\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import pandas_udf, col\n",
    "\n",
    "from utils import compute_centroidDistances, get_clusterId, get_minDistance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "67120e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a SparkSession \n",
    "#   connect to the master node on the port where the master node is listening (7077)\n",
    "#   declare the app name \n",
    "#   configure the executor memory to 512 MB\n",
    "#   either *connect* or *create* a new Spark Context\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"spark://spark-master:7077\")\\\n",
    "    .appName(\"kMeansParallel\")\\\n",
    "    .config(\"spark.executor.memory\", \"512m\")\\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext\n",
    "sc.addPyFile(\"utils.py\") # we need to export other files to the cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05cf1762",
   "metadata": {},
   "source": [
    "### Naming conventions\n",
    "\n",
    "The single datum is named as `datumName`, while the RDD that is a collection of one or more data is called `datumName_rdd`.\n",
    "\n",
    "Example: `compute_clusterDistances` returns `clusterDistances` (i.e. a numpy array of distances between a point `x` and the `centroids`). \n",
    "The RDD that collects all the `clusterDistances` will be called `clusterDistances_rdd`, and here is a sample implementation of that:\n",
    "```python\n",
    "def compute_centroidDistances(x, centroids):\n",
    "    return np.sum((centroids - x)**2, axis = 1)\n",
    "\n",
    "# `data_rdd` is an RDD\n",
    "# `centroids` is a numpy array\n",
    "clusterDistances_rdd = data_rdd \\\n",
    "    .map(lambda x: compute_clusterDistances(x, centroids))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5a86e34",
   "metadata": {},
   "source": [
    "### Load and preprocess the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9e8124d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Real dataset\n",
    "\n",
    "# Change percent10 to 'False' to fetch the full dataset (4M rows)\n",
    "# for local works is better to leave it as 'True'\n",
    "kdd = fetch_kddcup99(shuffle=True,percent10=True) \n",
    "kdd_data = kdd.data[:100_000]\n",
    "\n",
    "# Remove string features and standardize them\n",
    "data = np.delete(kdd_data,np.arange(1,4,1),axis = 1) \n",
    "scaler = StandardScaler()\n",
    "data = scaler.fit_transform(data)\n",
    "\n",
    "#parallelize\n",
    "data_rdd = sc.parallelize([row for row in data])\n",
    "data_rdd = data_rdd.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4600c02b",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_clusters = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f4edab4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kMeansPlusPlus_init(\n",
    "    data: npt.NDArray,\n",
    "    k: int,\n",
    "    weights: npt.NDArray = np.array([])\n",
    ") -> npt.NDArray:\n",
    "    \"\"\"\n",
    "    Standard kMeans++ initialization method:\n",
    "    given `data` (eventually weighted), returns `k` cluster centroids\n",
    "    \"\"\"\n",
    "    if weights.shape[0] == 0:\n",
    "        weights = np.ones(shape=(data.shape[0],1))\n",
    "    \n",
    "    centroids = data[np.random.randint(0, data.shape[0]),:].reshape(1, -1) # reshaping for easier stacking\n",
    "    \n",
    "    while (centroids.shape[0] < k):\n",
    "        # since the original functions are made for map\n",
    "        # we need to loop over the data\n",
    "        minDistance_array = np.array(\n",
    "            [get_minDistance(compute_centroidDistances(datum, centroids)) for datum in data]\n",
    "        ) * weights # multiplyling by the weight simulates multiple copies of the same datum\n",
    "        total_minDistance = np.sum(minDistance_array)\n",
    "        # sampling probability proportional to minDistance\n",
    "        new_centroid_idx = np.random.choice(minDistance_array.shape[0], size = 1, p = minDistance_array / total_minDistance)\n",
    "        new_centroid = data[new_centroid_idx,:].reshape(1, -1)\n",
    "\n",
    "        # edge case in which the same centroid is selected twice:\n",
    "        # redo the iteration without saving the centroid\n",
    "        if any(np.array_equal(new_centroid, row) for row in centroids): continue\n",
    "        centroids = np.concatenate((centroids, new_centroid), axis = 0)\n",
    "\n",
    "    return centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "49a63d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kMeansNaive(\n",
    "    data: npt.NDArray,\n",
    "    centroids: npt.NDArray,\n",
    "    epochs: int = 5\n",
    ") -> npt.NDArray:\n",
    "    \"\"\"\n",
    "    Standard kMeans algorithm:\n",
    "    given `data`, updates the (k) `centroids` for `epochs` times,\n",
    "    improving the clustering each time\n",
    "    \"\"\"\n",
    "    k = centroids.shape[0]\n",
    "    for _ in range(epochs):\n",
    "        assignments = np.array(\n",
    "            [get_clusterId(compute_centroidDistances(x, centroids)) for x in data]\n",
    "        )\n",
    "        centroids = np.array(\n",
    "            [np.mean(data[assignments==i,:], axis = 0) for i in range(k)]\n",
    "        )\n",
    "    return centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bb40ef7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kMeansParallel_init(\n",
    "    data_rdd: RDD,\n",
    "    k: int,\n",
    "    l: float\n",
    ") -> npt.NDArray:\n",
    "    \"\"\"\n",
    "    kMeans|| initialization method:\n",
    "    returns `k` good `centroids`.\n",
    "    `l` controls the probability of each point\n",
    "    in `data_rdd` of being sampled as a pre-processed centroid.\n",
    "    \"\"\"\n",
    "\n",
    "    centroids = np.array(\n",
    "        data_rdd.takeSample(num=1, withReplacement=False)\n",
    "    )\n",
    "    \n",
    "    minDistance_rdd = data_rdd \\\n",
    "        .map(lambda x: (x, get_minDistance(compute_centroidDistances(x, centroids)))) \\\n",
    "        .persist()\n",
    "\n",
    "    cost = minDistance_rdd \\\n",
    "        .map(lambda x: x[1]) \\\n",
    "        .sum()\n",
    "\n",
    "    iterations = int(np.ceil(np.log(cost))) if (cost > 1) else 1\n",
    "    for _ in range(iterations):\n",
    "        new_centroids = np.array(\n",
    "            minDistance_rdd \\\n",
    "                .filter(lambda x: np.random.rand() < np.min((l * x[1] / cost, 1))) \\\n",
    "                .map(lambda x: x[0]) \\\n",
    "                .collect()\n",
    "        )\n",
    "        # edge case in which no new centroid is sampled:\n",
    "        # this avoids the following `np.concatenate` to fail\n",
    "        if len(new_centroids.shape) < 2:\n",
    "            continue\n",
    "\n",
    "        minDistance_rdd.unpersist()\n",
    "        centroids = np.unique(\n",
    "            np.concatenate((centroids, new_centroids), axis = 0), \n",
    "            axis = 0\n",
    "        )\n",
    "\n",
    "        minDistance_rdd = data_rdd \\\n",
    "            .map(lambda x: (x, get_minDistance(compute_centroidDistances(x, centroids)))) \\\n",
    "            .persist()\n",
    "        cost = minDistance_rdd \\\n",
    "            .map(lambda x: x[1]) \\\n",
    "            .sum()\n",
    "    \n",
    "    minDistance_rdd.unpersist()\n",
    "    clusterCounts = data_rdd \\\n",
    "        .map(lambda x: (get_clusterId(compute_centroidDistances(x, centroids)), 1)) \\\n",
    "        .countByKey()\n",
    "    \n",
    "    clusterCounts = np.array([w[1] for w in clusterCounts.items()])\n",
    "    centroids = kMeansNaive(\n",
    "        centroids, \n",
    "        kMeansPlusPlus_init(centroids, k, clusterCounts)\n",
    "    )\n",
    "    \n",
    "    return centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "119cdd71",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/09/05 14:39:37 WARN TaskSetManager: Stage 0 contains a task of very large size (16530 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/09/05 14:39:40 WARN TaskSetManager: Stage 1 contains a task of very large size (16414 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/09/05 14:39:41 WARN TaskSetManager: Stage 2 contains a task of very large size (16530 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/09/05 14:39:43 WARN TaskSetManager: Stage 3 contains a task of very large size (16414 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/09/05 14:39:44 WARN TaskSetManager: Stage 4 contains a task of very large size (16530 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/09/05 14:39:46 WARN TaskSetManager: Stage 5 contains a task of very large size (16414 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/09/05 14:39:47 WARN TaskSetManager: Stage 6 contains a task of very large size (16530 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/09/05 14:39:50 WARN TaskSetManager: Stage 7 contains a task of very large size (16414 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/09/05 14:39:51 WARN TaskSetManager: Stage 8 contains a task of very large size (16414 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/09/05 14:39:54 WARN TaskSetManager: Stage 9 contains a task of very large size (16414 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/09/05 14:39:55 WARN TaskSetManager: Stage 10 contains a task of very large size (16414 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/09/05 14:39:57 WARN TaskSetManager: Stage 11 contains a task of very large size (16414 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/09/05 14:39:59 WARN TaskSetManager: Stage 12 contains a task of very large size (16530 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/09/05 14:40:02 WARN TaskSetManager: Stage 13 contains a task of very large size (16530 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/09/05 14:40:03 WARN TaskSetManager: Stage 14 contains a task of very large size (16414 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/09/05 14:40:06 WARN TaskSetManager: Stage 15 contains a task of very large size (16414 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/09/05 14:40:07 WARN TaskSetManager: Stage 16 contains a task of very large size (16530 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/09/05 14:40:10 WARN TaskSetManager: Stage 17 contains a task of very large size (16530 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/09/05 14:40:11 WARN TaskSetManager: Stage 18 contains a task of very large size (16414 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/09/05 14:40:15 WARN TaskSetManager: Stage 19 contains a task of very large size (16414 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/09/05 14:40:16 WARN TaskSetManager: Stage 20 contains a task of very large size (16530 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/09/05 14:40:20 WARN TaskSetManager: Stage 21 contains a task of very large size (16530 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/09/05 14:40:21 WARN TaskSetManager: Stage 22 contains a task of very large size (16530 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/09/05 14:40:26 WARN TaskSetManager: Stage 23 contains a task of very large size (16530 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/09/05 14:40:27 WARN TaskSetManager: Stage 24 contains a task of very large size (16414 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/09/05 14:40:32 WARN TaskSetManager: Stage 25 contains a task of very large size (16414 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/09/05 14:40:33 WARN TaskSetManager: Stage 26 contains a task of very large size (16414 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/09/05 14:40:38 WARN TaskSetManager: Stage 27 contains a task of very large size (16414 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/09/05 14:40:39 WARN TaskSetManager: Stage 28 contains a task of very large size (16414 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/09/05 14:40:46 WARN TaskSetManager: Stage 29 contains a task of very large size (16530 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/09/05 14:40:46 WARN TaskSetManager: Stage 30 contains a task of very large size (16530 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/09/05 14:40:53 WARN TaskSetManager: Stage 31 contains a task of very large size (16530 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/09/05 14:40:55 WARN TaskSetManager: Stage 32 contains a task of very large size (16414 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/09/05 14:41:01 WARN TaskSetManager: Stage 33 contains a task of very large size (16414 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/09/05 14:41:02 WARN TaskSetManager: Stage 34 contains a task of very large size (16414 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/09/05 14:41:08 WARN TaskSetManager: Stage 35 contains a task of very large size (16530 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/09/05 14:41:09 WARN TaskSetManager: Stage 36 contains a task of very large size (16530 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/09/05 14:41:16 WARN TaskSetManager: Stage 37 contains a task of very large size (16414 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "k = num_clusters\n",
    "l = k * 10\n",
    "centroids = kMeansParallel_init(data_rdd, k, l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "78407310",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndata = np.array(data)\\ncentroids = np.array(centroids)\\nfig, ax = plt.subplots()\\nax.scatter(data[:,0], data[:,1])\\nax.scatter(centroids[:,0], centroids[:,1])\\nfig.tight_layout()\\n'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "data = np.array(data)\n",
    "centroids = np.array(centroids)\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(data[:,0], data[:,1])\n",
    "ax.scatter(centroids[:,0], centroids[:,1])\n",
    "fig.tight_layout()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "07e96995",
   "metadata": {},
   "outputs": [],
   "source": [
    "def miniBatchKMeans(\n",
    "    data_rdd: RDD,\n",
    "    centroids: npt.NDArray,\n",
    "    iterations: int = 10,\n",
    "    batch_fraction: float = 0.1\n",
    ") -> npt.NDArray:\n",
    "    k = centroids.shape[0]\n",
    "    clusterCounters = np.zeros((k,)) # 1 / learning_rate\n",
    "    for iter in range(iterations):\n",
    "        miniBatch_rdd = data_rdd \\\n",
    "            .sample(withReplacement=False, fraction=batch_fraction)\n",
    "        miniBatch_rdd = miniBatch_rdd \\\n",
    "            .map(lambda x: (get_clusterId(compute_centroidDistances(x, centroids)), 1, x)) \\\n",
    "            .persist()\n",
    "        \n",
    "        # counting how many assigments per cluster\n",
    "        clusterCounts_dict = miniBatch_rdd \\\n",
    "            .map(lambda x: (x[0], x[1])) \\\n",
    "            .countByKey()\n",
    "        clusterCounts = np.array(\n",
    "            [clusterCounts_dict[i] if i in clusterCounts_dict.keys() else 0 for i in range(k)]\n",
    "        )\n",
    "        clusterCounters += clusterCounts\n",
    "        \n",
    "        # edge case in which a cluster has no assignments:\n",
    "        # if also its counter is zero the whole iteration is repeated\n",
    "        if any(np.isclose(v, 0) for v in clusterCounters): \n",
    "            iter -= 1\n",
    "            miniBatch_rdd.unpersist()\n",
    "            continue\n",
    "        # otherwise its count will be set to 1 to avoid division by 0 in the update step\n",
    "        clusterCounts = np.where(clusterCounts >= 1, clusterCounts, 1)\n",
    "\n",
    "        # summing all points assigned to the same cluster\n",
    "        # (in the update step this will be divided by the counts \n",
    "        # in order to get the mean for every cluster).\n",
    "        # A dict is used for convenience and consistency with clusterCounts\n",
    "        clusterSums_dict = dict(miniBatch_rdd \\\n",
    "            .map(lambda x: (x[0], x[2])) \\\n",
    "            .reduceByKey(lambda x, y: x + y) \\\n",
    "            .collect()\n",
    "        )\n",
    "        # edge case in which a cluster has no assignments:\n",
    "        # the centroid is returned instead of 0 \n",
    "        # (which would have been the sum of its assigned points) \n",
    "        # in order to not update its position \n",
    "        # (note how the terms cancel out in the update step)\n",
    "        clusterSums = np.array(\n",
    "            [clusterSums_dict[i] if i in clusterSums_dict.keys() else centroids[i,:] for i in range(k)]\n",
    "        )\n",
    "\n",
    "        # update step: c <- (1 - eta) * c + eta * x_mean\n",
    "        # (note x_mean = x_sums / c_count)\n",
    "        centroids = (1 - 1 / clusterCounters).reshape(-1, 1) * centroids + \\\n",
    "                    (1 / (clusterCounters * clusterCounts)).reshape(-1, 1) * clusterSums\n",
    "        \n",
    "        miniBatch_rdd.unpersist()\n",
    "        \n",
    "    return centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "30762a61",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonUtils.getBroadcastThreshold.\n: java.lang.NullPointerException: Cannot invoke \"org.apache.spark.api.java.JavaSparkContext.sc()\" because \"jsc\" is null\n\tat org.apache.spark.api.java.JavaSparkContext$.toSparkContext(JavaSparkContext.scala:794)\n\tat org.apache.spark.api.python.PythonUtils$.getBroadcastThreshold(PythonUtils.scala:87)\n\tat org.apache.spark.api.python.PythonUtils.getBroadcastThreshold(PythonUtils.scala)\n\tat jdk.internal.reflect.GeneratedMethodAccessor34.invoke(Unknown Source)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mPy4JJavaError\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[31]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m final_centroids = \u001b[43mminiBatchKMeans\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_rdd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcentroids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0.1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[30]\u001b[39m\u001b[32m, line 14\u001b[39m, in \u001b[36mminiBatchKMeans\u001b[39m\u001b[34m(data_rdd, centroids, iterations, batch_fraction)\u001b[39m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m \u001b[38;5;28miter\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(iterations):\n\u001b[32m     10\u001b[39m     miniBatch_rdd = data_rdd \\\n\u001b[32m     11\u001b[39m         .sample(withReplacement=\u001b[38;5;28;01mFalse\u001b[39;00m, fraction=batch_fraction)\n\u001b[32m     12\u001b[39m     miniBatch_rdd = \u001b[43mminiBatch_rdd\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m        \u001b[49m\u001b[43m.\u001b[49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mget_clusterId\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcompute_centroidDistances\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcentroids\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m \u001b[43m        \u001b[49m\u001b[43m.\u001b[49m\u001b[43mpersist\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     16\u001b[39m     \u001b[38;5;66;03m# counting how many assigments per cluster\u001b[39;00m\n\u001b[32m     17\u001b[39m     clusterCounts_dict = miniBatch_rdd \\\n\u001b[32m     18\u001b[39m         .map(\u001b[38;5;28;01mlambda\u001b[39;00m x: (x[\u001b[32m0\u001b[39m], x[\u001b[32m1\u001b[39m])) \\\n\u001b[32m     19\u001b[39m         .countByKey()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/bin/spark-3.5.5-bin-hadoop3/python/pyspark/rdd.py:509\u001b[39m, in \u001b[36mRDD.persist\u001b[39m\u001b[34m(self, storageLevel)\u001b[39m\n\u001b[32m    507\u001b[39m \u001b[38;5;28mself\u001b[39m.is_cached = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    508\u001b[39m javaStorageLevel = \u001b[38;5;28mself\u001b[39m.ctx._getJavaStorageLevel(storageLevel)\n\u001b[32m--> \u001b[39m\u001b[32m509\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_jrdd\u001b[49m.persist(javaStorageLevel)\n\u001b[32m    510\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/bin/spark-3.5.5-bin-hadoop3/python/pyspark/rdd.py:5470\u001b[39m, in \u001b[36mPipelinedRDD._jrdd\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   5467\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   5468\u001b[39m     profiler = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m5470\u001b[39m wrapped_func = \u001b[43m_wrap_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   5471\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_prev_jrdd_deserializer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_jrdd_deserializer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprofiler\u001b[49m\n\u001b[32m   5472\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   5474\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.ctx._jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   5475\u001b[39m python_rdd = \u001b[38;5;28mself\u001b[39m.ctx._jvm.PythonRDD(\n\u001b[32m   5476\u001b[39m     \u001b[38;5;28mself\u001b[39m._prev_jrdd.rdd(), wrapped_func, \u001b[38;5;28mself\u001b[39m.preservesPartitioning, \u001b[38;5;28mself\u001b[39m.is_barrier\n\u001b[32m   5477\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/bin/spark-3.5.5-bin-hadoop3/python/pyspark/rdd.py:5268\u001b[39m, in \u001b[36m_wrap_function\u001b[39m\u001b[34m(sc, func, deserializer, serializer, profiler)\u001b[39m\n\u001b[32m   5266\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m serializer, \u001b[33m\"\u001b[39m\u001b[33mserializer should not be empty\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   5267\u001b[39m command = (func, profiler, deserializer, serializer)\n\u001b[32m-> \u001b[39m\u001b[32m5268\u001b[39m pickled_command, broadcast_vars, env, includes = \u001b[43m_prepare_for_python_RDD\u001b[49m\u001b[43m(\u001b[49m\u001b[43msc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   5269\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m sc._jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   5270\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m sc._jvm.SimplePythonFunction(\n\u001b[32m   5271\u001b[39m     \u001b[38;5;28mbytearray\u001b[39m(pickled_command),\n\u001b[32m   5272\u001b[39m     env,\n\u001b[32m   (...)\u001b[39m\u001b[32m   5277\u001b[39m     sc._javaAccumulator,\n\u001b[32m   5278\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/bin/spark-3.5.5-bin-hadoop3/python/pyspark/rdd.py:5253\u001b[39m, in \u001b[36m_prepare_for_python_RDD\u001b[39m\u001b[34m(sc, command)\u001b[39m\n\u001b[32m   5251\u001b[39m pickled_command = ser.dumps(command)\n\u001b[32m   5252\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m sc._jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m5253\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(pickled_command) > \u001b[43msc\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_jvm\u001b[49m\u001b[43m.\u001b[49m\u001b[43mPythonUtils\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgetBroadcastThreshold\u001b[49m\u001b[43m(\u001b[49m\u001b[43msc\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_jsc\u001b[49m\u001b[43m)\u001b[49m:  \u001b[38;5;66;03m# Default 1M\u001b[39;00m\n\u001b[32m   5254\u001b[39m     \u001b[38;5;66;03m# The broadcast will have same life cycle as created PythonRDD\u001b[39;00m\n\u001b[32m   5255\u001b[39m     broadcast = sc.broadcast(pickled_command)\n\u001b[32m   5256\u001b[39m     pickled_command = ser.dumps(broadcast)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/bin/spark-3.5.5-bin-hadoop3/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1322\u001b[39m, in \u001b[36mJavaMember.__call__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m   1316\u001b[39m command = proto.CALL_COMMAND_NAME +\\\n\u001b[32m   1317\u001b[39m     \u001b[38;5;28mself\u001b[39m.command_header +\\\n\u001b[32m   1318\u001b[39m     args_command +\\\n\u001b[32m   1319\u001b[39m     proto.END_COMMAND_PART\n\u001b[32m   1321\u001b[39m answer = \u001b[38;5;28mself\u001b[39m.gateway_client.send_command(command)\n\u001b[32m-> \u001b[39m\u001b[32m1322\u001b[39m return_value = \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1323\u001b[39m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1325\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[32m   1326\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[33m\"\u001b[39m\u001b[33m_detach\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/bin/spark-3.5.5-bin-hadoop3/python/pyspark/errors/exceptions/captured.py:179\u001b[39m, in \u001b[36mcapture_sql_exception.<locals>.deco\u001b[39m\u001b[34m(*a, **kw)\u001b[39m\n\u001b[32m    177\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdeco\u001b[39m(*a: Any, **kw: Any) -> Any:\n\u001b[32m    178\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m179\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    180\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    181\u001b[39m         converted = convert_exception(e.java_exception)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/bin/spark-3.5.5-bin-hadoop3/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py:326\u001b[39m, in \u001b[36mget_return_value\u001b[39m\u001b[34m(answer, gateway_client, target_id, name)\u001b[39m\n\u001b[32m    324\u001b[39m value = OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[32m2\u001b[39m:], gateway_client)\n\u001b[32m    325\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[32m1\u001b[39m] == REFERENCE_TYPE:\n\u001b[32m--> \u001b[39m\u001b[32m326\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[32m    327\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.\n\u001b[32m    328\u001b[39m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m, name), value)\n\u001b[32m    329\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    330\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[32m    331\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[33m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.\n\u001b[32m    332\u001b[39m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m, name, value))\n",
      "\u001b[31mPy4JJavaError\u001b[39m: An error occurred while calling z:org.apache.spark.api.python.PythonUtils.getBroadcastThreshold.\n: java.lang.NullPointerException: Cannot invoke \"org.apache.spark.api.java.JavaSparkContext.sc()\" because \"jsc\" is null\n\tat org.apache.spark.api.java.JavaSparkContext$.toSparkContext(JavaSparkContext.scala:794)\n\tat org.apache.spark.api.python.PythonUtils$.getBroadcastThreshold(PythonUtils.scala:87)\n\tat org.apache.spark.api.python.PythonUtils.getBroadcastThreshold(PythonUtils.scala)\n\tat jdk.internal.reflect.GeneratedMethodAccessor34.invoke(Unknown Source)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\n"
     ]
    }
   ],
   "source": [
    "final_centroids = miniBatchKMeans(data_rdd, centroids, 10, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "df940414",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndata = np.array(data)\\ncentroids = np.array(centroids)\\nfig, ax = plt.subplots()\\nax.scatter(data[:,0], data[:,1])\\nax.scatter(centroids[:,0], centroids[:,1])\\nax.scatter(final_centroids[:,0], final_centroids[:,1])\\nfig.tight_layout()\\n'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "data = np.array(data)\n",
    "centroids = np.array(centroids)\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(data[:,0], data[:,1])\n",
    "ax.scatter(centroids[:,0], centroids[:,1])\n",
    "ax.scatter(final_centroids[:,0], final_centroids[:,1])\n",
    "fig.tight_layout()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2abe2558",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
