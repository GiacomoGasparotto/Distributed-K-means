{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cb95c923",
   "metadata": {},
   "outputs": [],
   "source": [
    "# general imports\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import numpy.typing as npt\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "from functools import singledispatch\n",
    "\n",
    "# dataset\n",
    "from sklearn.datasets import fetch_kddcup99\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# pyspark module\n",
    "from pyspark.rdd import RDD\n",
    "\n",
    "# src module\n",
    "from src.utils import sparkSetup\n",
    "from src.kmeans import compute_centroidDistances, get_clusterId, get_minDistance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "08e6fa38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the zipped environment if it doesn't already exist\n",
    "! if [ ! -f \"environment.tar.gz\" ]; then venv-pack -o \"environment.tar.gz\" ; fi\n",
    "# creating the zipped module src\n",
    "! if [ -f \"src.tar.gz\" ]; then rm src.tar.gz ; fi\n",
    "! tar -czf src.tar.gz src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3bf3e878",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting org.apache.spark.deploy.master.Master, logging to /usr/local/spark/logs/spark-ubuntu-org.apache.spark.deploy.master.Master-1-mapd-b-14-1.out\n",
      "master: starting org.apache.spark.deploy.worker.Worker, logging to /usr/local/spark/logs/spark-ubuntu-org.apache.spark.deploy.worker.Worker-1-mapd-b-14-1.out\n",
      "worker1: starting org.apache.spark.deploy.worker.Worker, logging to /usr/local/spark/logs/spark-ubuntu-org.apache.spark.deploy.worker.Worker-1-mapd-b-14-2.out\n",
      "worker3: starting org.apache.spark.deploy.worker.Worker, logging to /usr/local/spark/logs/spark-ubuntu-org.apache.spark.deploy.worker.Worker-1-mapd-b-14-4.out\n",
      "worker2: starting org.apache.spark.deploy.worker.Worker, logging to /usr/local/spark/logs/spark-ubuntu-org.apache.spark.deploy.worker.Worker-1-mapd-b-14-3.out\n"
     ]
    }
   ],
   "source": [
    "# starting the cluster\n",
    "! $SPARK_HOME/sbin/start-all.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f5f8c42b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# telling spark where to find the python binary\n",
    "os.environ[\"PYSPARK_PYTHON\"] = \"./environment/bin/python\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "56bd4ed6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/09/10 08:51:46 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/09/10 08:51:56 WARN Utils: Untarring behavior will be deprecated at spark.files and SparkContext.addFile. Consider using spark.archives or SparkContext.addArchive instead.\n"
     ]
    }
   ],
   "source": [
    "# creating a sparkSession\n",
    "spark = sparkSetup(\"kMeans\")\n",
    "sc = spark.sparkContext\n",
    "# exporting the src module\n",
    "sc.addPyFile(\"src.tar.gz\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05cf1762",
   "metadata": {},
   "source": [
    "### Naming conventions\n",
    "\n",
    "The single datum is named as `datumName`, while the RDD that is a collection of one or more data is called `datumName_rdd`.\n",
    "\n",
    "Example: `compute_clusterDistances` returns `clusterDistances` (i.e. a numpy array of distances between a point `x` and the `centroids`). \n",
    "The RDD that collects all the `clusterDistances` will be called `clusterDistances_rdd`, and here is a sample implementation of that:\n",
    "```python\n",
    "def compute_centroidDistances(x, centroids):\n",
    "    return np.sum((centroids - x)**2, axis = 1)\n",
    "\n",
    "# `data_rdd` is an RDD\n",
    "# `centroids` is a numpy array\n",
    "clusterDistances_rdd = data_rdd \\\n",
    "    .map(lambda x: compute_clusterDistances(x, centroids))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5a86e34",
   "metadata": {},
   "source": [
    "### Load and preprocess the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a245d743",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fetch the dataset and its labels\n",
    "kdd_data, kdd_labels = fetch_kddcup99(\n",
    "    percent10 = True,\n",
    "    shuffle = True,\n",
    "    return_X_y = True\n",
    ")\n",
    "# transform bytes entries into integers\n",
    "entries_dict = {\n",
    "    i: np.unique(kdd_data[:,i], return_inverse=True) \n",
    "        for i in range(kdd_data.shape[1]) \n",
    "            if isinstance(kdd_data[0,i], bytes) \n",
    "}\n",
    "for key, values in entries_dict.items():\n",
    "    kdd_data[:,key] = values[1]\n",
    "# and then cast everything into a float\n",
    "kdd_data = kdd_data.astype(float)\n",
    "\n",
    "# get the number of clusters from kdd_labels\n",
    "k = np.unique(kdd_labels).shape[0]\n",
    "\n",
    "# standardizing the dataset\n",
    "scaler = StandardScaler()\n",
    "data = scaler.fit_transform(kdd_data)\n",
    "\n",
    "#parallelize\n",
    "data_rdd = sc.parallelize([row for row in data])\n",
    "data_rdd = data_rdd.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "93b22364",
   "metadata": {},
   "outputs": [],
   "source": [
    "@singledispatch\n",
    "def kMeansRandom_init(\n",
    "    data: RDD | npt.NDArray ,\n",
    "    k: int\n",
    ") -> npt.NDArray:\n",
    "    \"\"\"\n",
    "    prova\n",
    "    \"\"\"\n",
    "    raise TypeError(\"Unsupported data type\")\n",
    "\n",
    "@kMeansRandom_init.register(RDD)\n",
    "def _(\n",
    "    data: RDD,\n",
    "    k: int\n",
    ") -> npt.NDArray:\n",
    "    \"\"\"\n",
    "    prova1\n",
    "    \"\"\"\n",
    "    centroids = np.array(\n",
    "        data.takeSample(withReplacement=False, num=k)\n",
    "    )\n",
    "    return centroids\n",
    "\n",
    "@kMeansRandom_init.register(np.ndarray)\n",
    "def _(\n",
    "    data: npt.NDArray,\n",
    "    k: int\n",
    ") -> npt.NDArray:\n",
    "    \"\"\"\n",
    "    prova2\n",
    "    \"\"\"\n",
    "    centroids = data[np.random.choice(data.shape[0], size = k), :]\n",
    "    return centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f4edab4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kMeansPlusPlus_init(\n",
    "    data: npt.NDArray,\n",
    "    k: int,\n",
    "    weights: npt.NDArray = np.array([])\n",
    ") -> npt.NDArray:\n",
    "    \"\"\"\n",
    "    Standard kMeans++ initialization method:\n",
    "    given `data` (eventually weighted), returns `k` cluster centroids\n",
    "    \"\"\"\n",
    "    if weights.shape[0] == 0:\n",
    "        weights = np.ones(shape=(data.shape[0],1))\n",
    "    \n",
    "    centroids = data[np.random.randint(0, data.shape[0]),:].reshape(1, -1) # reshaping for easier stacking\n",
    "    \n",
    "    while (centroids.shape[0] < k):\n",
    "        # since the original functions are made for map\n",
    "        # we need to loop over the data\n",
    "        minDistance_array = np.array(\n",
    "            [get_minDistance(compute_centroidDistances(datum, centroids)) for datum in data]\n",
    "        ) * weights # multiplyling by the weight simulates multiple copies of the same datum\n",
    "        total_minDistance = np.sum(minDistance_array)\n",
    "        # sampling probability proportional to minDistance\n",
    "        new_centroid_idx = np.random.choice(minDistance_array.shape[0], size = 1, p = minDistance_array / total_minDistance)\n",
    "        new_centroid = data[new_centroid_idx,:].reshape(1, -1)\n",
    "\n",
    "        # edge case in which the same centroid is selected twice:\n",
    "        # redo the iteration without saving the centroid\n",
    "        if any(np.array_equal(new_centroid, row) for row in centroids): continue\n",
    "        centroids = np.concatenate((centroids, new_centroid), axis = 0)\n",
    "\n",
    "    return centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "49a63d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kMeansNaive(\n",
    "    data: npt.NDArray,\n",
    "    centroids: npt.NDArray,\n",
    "    epochs: int = 5\n",
    ") -> npt.NDArray:\n",
    "    \"\"\"\n",
    "    Standard kMeans algorithm:\n",
    "    given `data`, updates the (k) `centroids` for `epochs` times,\n",
    "    improving the clustering each time\n",
    "    \"\"\"\n",
    "    k = centroids.shape[0]\n",
    "    for _ in range(epochs):\n",
    "        assignments = np.array(\n",
    "            [get_clusterId(compute_centroidDistances(x, centroids)) for x in data]\n",
    "        )\n",
    "        centroids = np.array(\n",
    "            [np.mean(data[assignments==i,:], axis = 0) for i in range(k)]\n",
    "        )\n",
    "    return centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bb40ef7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kMeansParallel_init(\n",
    "    data_rdd: RDD,\n",
    "    k: int,\n",
    "    l: float\n",
    ") -> npt.NDArray:\n",
    "    \"\"\"\n",
    "    kMeans|| initialization method:\n",
    "    returns `k` good `centroids`.\n",
    "    `l` controls the probability of each point\n",
    "    in `data_rdd` of being sampled as a pre-processed centroid.\n",
    "    \"\"\"\n",
    "\n",
    "    centroids = np.array(\n",
    "        data_rdd.takeSample(num=1, withReplacement=False)\n",
    "    )\n",
    "    \n",
    "    minDistance_rdd = data_rdd \\\n",
    "        .map(lambda x: (x, get_minDistance(compute_centroidDistances(x, centroids)))) \\\n",
    "        .persist()\n",
    "\n",
    "    cost = minDistance_rdd \\\n",
    "        .map(lambda x: x[1]) \\\n",
    "        .sum()\n",
    "\n",
    "    iterations = int(np.ceil(np.log(cost))) if (cost > 1) else 1\n",
    "    for _ in range(iterations):\n",
    "        new_centroids = np.array(\n",
    "            minDistance_rdd \\\n",
    "                .filter(lambda x: np.random.rand() < np.min((l * x[1] / cost, 1))) \\\n",
    "                .map(lambda x: x[0]) \\\n",
    "                .collect()\n",
    "        )\n",
    "        # edge case in which no new centroid is sampled:\n",
    "        # this avoids the following `np.concatenate` to fail\n",
    "        if len(new_centroids.shape) < 2:\n",
    "            continue\n",
    "\n",
    "        minDistance_rdd.unpersist()\n",
    "        centroids = np.unique(\n",
    "            np.concatenate((centroids, new_centroids), axis = 0), \n",
    "            axis = 0\n",
    "        )\n",
    "\n",
    "        minDistance_rdd = data_rdd \\\n",
    "            .map(lambda x: (x, get_minDistance(compute_centroidDistances(x, centroids)))) \\\n",
    "            .persist()\n",
    "        cost = minDistance_rdd \\\n",
    "            .map(lambda x: x[1]) \\\n",
    "            .sum()\n",
    "    \n",
    "    minDistance_rdd.unpersist()\n",
    "    clusterCounts = data_rdd \\\n",
    "        .map(lambda x: (get_clusterId(compute_centroidDistances(x, centroids)), 1)) \\\n",
    "        .countByKey()\n",
    "    \n",
    "    clusterCounts = np.array([w[1] for w in clusterCounts.items()])\n",
    "    centroids = kMeansNaive(\n",
    "        centroids, \n",
    "        kMeansPlusPlus_init(centroids, k, clusterCounts)\n",
    "    )\n",
    "    \n",
    "    return centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "119cdd71",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/09/09 08:28:44 WARN TaskSetManager: Stage 24 contains a task of very large size (10124 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/09/09 08:28:45 WARN TaskSetManager: Stage 25 contains a task of very large size (10124 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "k = 15\n",
    "l = k * 10\n",
    "centroids = kMeansParallel_init(data_rdd, k, l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7582012d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_kmeans_parallel(data_rdd: RDD,\n",
    "    centroids: npt.NDArray,\n",
    "    iterations: int = 10,\n",
    "    batch_fraction: float = 0.1\n",
    ") -> npt.NDArray:\n",
    "    \"\"\"\n",
    "    Parallelized K-Means with weighted centroid updates (weights = counts for each class).\n",
    "    \"\"\"\n",
    "    k = centroids.shape[0]\n",
    "    for _ in range(iterations):\n",
    "        # assign each point to the closest cluster\n",
    "        data_assigned_rdd = data_rdd \\\n",
    "            .map(lambda x: (get_clusterId(compute_centroidDistances(x, centroids)), 1, x)) \\\n",
    "            .persist()\n",
    "        \n",
    "        # counting how many assigments per cluster\n",
    "        clusterWeights_dict = data_assigned_rdd \\\n",
    "            .map(lambda x: (x[0], x[1])) \\\n",
    "            .countByKey()\n",
    "        \n",
    "        # compute the numerator term (weights * coordinates)\n",
    "        clusterWeightedSums_dict = dict(data_assigned_rdd \\\n",
    "                .map(lambda x: (x[0], x[2])) \\\n",
    "                .reduceByKey(lambda x, y: x + y) \\\n",
    "                .collect()\n",
    "        )\n",
    "\n",
    "        # compute the weighted average (they are the updated clusters). If no counts maintain the older centroid values\n",
    "        centroids = np.array(\n",
    "                [clusterWeightedSums_dict[i]/clusterWeights_dict[i] \n",
    "                if i in clusterWeights_dict.keys() else centroids[i,:]\n",
    "                for i in range(k)]\n",
    "            )\n",
    "        \n",
    "        # free memory\n",
    "        data_assigned_rdd.unpersist()\n",
    "        \n",
    "    return centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07e96995",
   "metadata": {},
   "outputs": [],
   "source": [
    "def miniBatchKMeans(\n",
    "    data_rdd: RDD,\n",
    "    centroids: npt.NDArray,\n",
    "    iterations: int = 10,\n",
    "    batch_fraction: float = 0.1\n",
    ") -> npt.NDArray:\n",
    "    \"\"\"\n",
    "    Mini-batch K-Means implementation with exponential averaging for centroid updates.\n",
    "    \"\"\"\n",
    "    k = centroids.shape[0]\n",
    "    clusterCounters = np.zeros((k,)) # 1 / learning_rate\n",
    "    for iter in range(iterations):\n",
    "        miniBatch_rdd = data_rdd \\\n",
    "            .sample(withReplacement=False, fraction=batch_fraction)\n",
    "        miniBatch_rdd = miniBatch_rdd \\\n",
    "            .map(lambda x: (get_clusterId(compute_centroidDistances(x, centroids)), 1, x)) \\\n",
    "            .persist()\n",
    "        \n",
    "        # counting how many assigments per cluster\n",
    "        clusterCounts_dict = miniBatch_rdd \\\n",
    "            .map(lambda x: (x[0], x[1])) \\\n",
    "            .countByKey()\n",
    "        clusterCounts = np.array(\n",
    "            [clusterCounts_dict[i] if i in clusterCounts_dict.keys() else 0 for i in range(k)]\n",
    "        )\n",
    "        clusterCounters += clusterCounts\n",
    "        \n",
    "        # edge case in which a cluster has no assignments:\n",
    "        # if also its counter is zero the whole iteration is repeated\n",
    "        if any(np.isclose(v, 0) for v in clusterCounters): \n",
    "            iter -= 1\n",
    "            miniBatch_rdd.unpersist()\n",
    "            continue\n",
    "        # otherwise its count will be set to 1 to avoid division by 0 in the update step\n",
    "        clusterCounts = np.where(clusterCounts >= 1, clusterCounts, 1)\n",
    "\n",
    "        # summing all points assigned to the same cluster\n",
    "        # (in the update step this will be divided by the counts \n",
    "        # in order to get the mean for every cluster).\n",
    "        # A dict is used for convenience and consistency with clusterCounts\n",
    "        clusterSums_dict = dict(miniBatch_rdd \\\n",
    "            .map(lambda x: (x[0], x[2])) \\\n",
    "            .reduceByKey(lambda x, y: x + y) \\\n",
    "            .collect()\n",
    "        )\n",
    "        # edge case in which a cluster has no assignments:\n",
    "        # the centroid is returned instead of 0 \n",
    "        # (which would have been the sum of its assigned points) \n",
    "        # in order to not update its position \n",
    "        # (note how the terms cancel out in the update step)\n",
    "        clusterSums = np.array(\n",
    "            [clusterSums_dict[i] if i in clusterSums_dict.keys() else centroids[i,:] for i in range(k)]\n",
    "        )\n",
    "\n",
    "        # update step: c <- (1 - eta) * c + eta * x_mean\n",
    "        # (note x_mean = x_sums / c_count)\n",
    "        centroids = (1 - 1 / clusterCounters).reshape(-1, 1) * centroids + \\\n",
    "                    (1 / (clusterCounters * clusterCounts)).reshape(-1, 1) * clusterSums\n",
    "        \n",
    "        miniBatch_rdd.unpersist()\n",
    "        \n",
    "    return centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "30762a61",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/09/08 08:28:23 WARN TaskSetManager: Stage 40 contains a task of very large size (10124 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/09/08 08:28:24 WARN TaskSetManager: Stage 41 contains a task of very large size (10124 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/09/08 08:28:24 WARN TaskSetManager: Stage 42 contains a task of very large size (10124 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/09/08 08:28:25 WARN TaskSetManager: Stage 43 contains a task of very large size (10124 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/09/08 08:28:26 WARN TaskSetManager: Stage 44 contains a task of very large size (10124 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/09/08 08:28:27 WARN TaskSetManager: Stage 45 contains a task of very large size (10124 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/09/08 08:28:28 WARN TaskSetManager: Stage 46 contains a task of very large size (10124 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/09/08 08:28:29 WARN TaskSetManager: Stage 47 contains a task of very large size (10124 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/09/08 08:28:30 WARN TaskSetManager: Stage 48 contains a task of very large size (10124 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/09/08 08:28:30 WARN TaskSetManager: Stage 49 contains a task of very large size (10124 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "final_centroids = miniBatchKMeans(data_rdd, centroids, 10, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2abe2558",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6f852301",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "worker1: stopping org.apache.spark.deploy.worker.Worker\n",
      "worker3: stopping org.apache.spark.deploy.worker.Worker\n",
      "worker2: stopping org.apache.spark.deploy.worker.Worker\n",
      "master: stopping org.apache.spark.deploy.worker.Worker\n",
      "stopping org.apache.spark.deploy.master.Master\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/09/10 09:18:57 WARN StandaloneAppClient$ClientEndpoint: Connection to 10.67.22.224:7077 failed; waiting for master to reconnect...\n",
      "25/09/10 09:18:57 WARN StandaloneSchedulerBackend: Disconnected from Spark cluster! Waiting for reconnection...\n",
      "25/09/10 09:19:02 ERROR TaskSchedulerImpl: Lost executor 0 on 10.67.22.170: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.\n",
      "25/09/10 09:19:02 ERROR TaskSchedulerImpl: Lost executor 1 on 10.67.22.208: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.\n",
      "25/09/10 09:19:02 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_0_11 !\n",
      "25/09/10 09:19:02 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_0_15 !\n",
      "25/09/10 09:19:02 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_0_7 !\n",
      "25/09/10 09:19:02 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_0_3 !\n",
      "25/09/10 09:19:02 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_0_12 !\n",
      "25/09/10 09:19:02 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_0_4 !\n",
      "25/09/10 09:19:02 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_0_0 !\n",
      "25/09/10 09:19:02 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_0_8 !\n",
      "25/09/10 09:19:02 ERROR TaskSchedulerImpl: Lost executor 2 on 10.67.22.202: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.\n",
      "25/09/10 09:19:02 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_0_2 !\n",
      "25/09/10 09:19:02 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_0_10 !\n",
      "25/09/10 09:19:02 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_0_6 !\n",
      "25/09/10 09:19:02 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_0_14 !\n",
      "25/09/10 09:19:02 ERROR TaskSchedulerImpl: Lost executor 3 on 10.67.22.224: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.\n",
      "25/09/10 09:19:02 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_0_9 !\n",
      "25/09/10 09:19:02 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_0_5 !\n",
      "25/09/10 09:19:02 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_0_1 !\n",
      "25/09/10 09:19:02 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_0_13 !\n"
     ]
    }
   ],
   "source": [
    "# stopping the cluster\n",
    "! $SPARK_HOME/sbin/stop-all.sh"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
