{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# general imports\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import numpy.typing as npt\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "from functools import singledispatch\n",
    "\n",
    "# dataset\n",
    "from sklearn.datasets import fetch_kddcup99\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# pyspark module\n",
    "from pyspark.rdd import RDD\n",
    "\n",
    "# src module\n",
    "from src.utils import kddSetup, sparkSetup\n",
    "from src.kmeans import compute_centroidDistances, get_clusterId, get_minDistance, kMeansPlusPlus_init, kMeansRandom_init, miniBatchKMeans, naiveKMeans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start a Spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the zipped environment if it doesn't already exist\n",
    "! if [ ! -f \"environment.tar.gz\" ]; then venv-pack -o \"environment.tar.gz\" ; fi\n",
    "# creating the zipped module src\n",
    "! if [ -f \"src.tar.gz\" ]; then rm src.tar.gz ; fi\n",
    "! tar -czf src.tar.gz src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# starting the cluster\n",
    "! $SPARK_HOME/sbin/start-all.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"PYSPARK_PYTHON\"] = \"./environment/bin/python\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a sparkSession\n",
    "spark = sparkSetup(\"kMeans\")\n",
    "sc = spark.sparkContext\n",
    "# exporting the src module\n",
    "sc.addPyFile(\"src.tar.gz\") # telling spark where to find the python binary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and preprocess the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first dataset we would like to test is a synthetic GaussMixture. To generate it, we sampled kcenters from a 15-dimensional spherical Gaussian distribution with mean at the origin and variance Râˆˆ{1,10,100}. We then added points from Gaussian distributions of unit variance around each center. Given the k centers, this is a mixture of k spherical Gaussians with equal weights.\n",
    "\n",
    "ref{paper kmeans||}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gauss_mixture(\n",
    "    num_points_per_cluster = 50,  \n",
    "    num_clusters = 10, # k centers\n",
    "    dim = 15,                  \n",
    "    R = 10 # center variances\n",
    "):\n",
    "    seed = 42\n",
    "    np.random.seed(seed)\n",
    "    # Centers generation N(0, R*I)\n",
    "    centers = np.random.normal(loc=0, scale=np.sqrt(R), size=(num_clusters, dim))\n",
    "    # Point generation N(center, I) for each cluster\n",
    "    data = np.concatenate(\n",
    "        [center + np.random.randn(num_points_per_cluster, dim) for center in centers],\n",
    "        axis=0\n",
    "    )\n",
    "    # Distributes and stores in memory data across worker nodes\n",
    "    gm_data_rdd = sc.parallelize([row for row in data])\n",
    "    gm_data_rdd = gm_data_rdd.persist()\n",
    "    \n",
    "    return gm_data_rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try to generate data\n",
    "gm10_data_rdd = gauss_mixture()  # R=10, k=10, shape=(500, 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_elements = gm10_data_rdd.count()\n",
    "element_dim = len(gm10_data_rdd.first())\n",
    "print(f\"Shape of RDD object: ({num_elements}, {element_dim})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to numpy array\n",
    "gm10_data_list = gm10_data_rdd.collect()\n",
    "gm10_data_array = np.array(gm10_data_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gm10_df = pd.DataFrame(gm10_data_array)\n",
    "gm10_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 10\n",
    "l = 2*k\n",
    "centroids = kMeansParallel_init(gm10_data_rdd, k, l)\n",
    "\n",
    "print(centroids.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_centroids = miniBatchKMeans(gm10_data_rdd, centroids)\n",
    "\n",
    "print(final_centroids.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KDD dataset\n",
    "# Change percent10 to 'False' to fetch the full dataset (4M rows)\n",
    "kdd = fetch_kddcup99(shuffle=True, percent10=True) \n",
    "kdd_data = kdd.data\n",
    "\n",
    "# Remove string features and standardize them\n",
    "data_kdd = np.delete(kdd_data,np.arange(1,4,1),axis = 1) \n",
    "scaler_kdd = StandardScaler()\n",
    "data_kdd = scaler_kdd.fit_transform(data_kdd)\n",
    "\n",
    "# Parallelize\n",
    "kdd_data_rdd = sc.parallelize([row for row in data_kdd])\n",
    "# data_rdd = sc.parallelize(np.array(data).tolist(), numSlices=16)\n",
    "kdd_data_rdd = kdd_data_rdd.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_elements = kdd_data_rdd.count()\n",
    "element_dim = len(kdd_data_rdd.first())\n",
    "print(f\"Shape of RDD object: ({num_elements}, {element_dim})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost_function(data: RDD, centroids: npt.ArrayLike) -> float:\n",
    "    \"\"\"\n",
    "    Compute cost function: square distance between data and centroids\n",
    "    data: (N, d)\n",
    "    centroids: (k, d)\n",
    "    \"\"\"\n",
    "    minDistance_rdd = data \\\n",
    "        .map(lambda x: (x, get_minDistance(compute_centroidDistances(x, centroids)))) \\\n",
    "        .persist()\n",
    "    cost = minDistance_rdd \\\n",
    "        .map(lambda x: x[1]) \\\n",
    "        .sum()\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analysis_init(\n",
    "        data_array: npt.ArrayLike, \n",
    "        data_rdd: RDD, \n",
    "        r: int, \n",
    "        k: int, \n",
    "        iterations: int=10, \n",
    "        batch_fraction: float=0.1\n",
    "        ) -> npt.DTypeLike:\n",
    "    results = []\n",
    "    \"\"\"\"\n",
    "    Computes performance tracking metrics for different initialization algorithms with *seed cost* (after initialization) and *final cost* (after miniBatchKmean iterations)\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    # 1 - Random initialization\n",
    "    start = time.time()\n",
    "    centroidsRandom = data_array[np.random.choice(data_array.shape[0], size=k, replace=False)]\n",
    "    init_time = time.time() - start\n",
    "\n",
    "    seed_cost = cost_function(data_rdd, centroidsRandom)\n",
    "\n",
    "    #final_centroids = miniBatchKMeans(data_rdd, centroidsRandom, iterations, batch_fraction)\n",
    "    final_centroids = kMeansNaive(data_array, centroidsRandom, iterations)\n",
    "\n",
    "    final_cost = cost_function(data_rdd, final_centroids)\n",
    "    \n",
    "    results.append({\n",
    "        \"method\": \"random\",\n",
    "        \"k\": k, \n",
    "        \"l\": np.nan,\n",
    "        \"r\": r, \n",
    "        \"initialization_time\": init_time,\n",
    "        \"seed\": seed_cost,\n",
    "        \"final\": final_cost\n",
    "    })\n",
    "\n",
    "\n",
    "    # 2 - k-means++\n",
    "    start = time.time()\n",
    "    centroidsPlusPlus = kMeansPlusPlus_init(data_array, k)\n",
    "    init_time = time.time() - start\n",
    "\n",
    "    seed_cost = cost_function(data_rdd, centroidsPlusPlus)\n",
    "    #final_centroids = miniBatchKMeans(data_rdd, centroidsPlusPlus, iterations, batch_fraction)\n",
    "    final_centroids = kMeansNaive(data_array, centroidsRandom, iterations)\n",
    "\n",
    "    final_cost = cost_function(data_rdd, final_centroids)\n",
    "\n",
    "    results.append({\n",
    "        \"method\": \"kmeans++\",\n",
    "        \"k\": k,\n",
    "        \"l\": np.nan,\n",
    "        \"r\": r, \n",
    "        \"initialization_time\": init_time,\n",
    "        \"seed\": seed_cost,\n",
    "        \"final\": final_cost\n",
    "    })\n",
    "\n",
    "\n",
    "    # 3 - Parallel l=0.5k\n",
    "    start = time.time()\n",
    "    centroidsParallel1 = kMeansParallel_init(data_rdd, k=k, l=0.5*k, r=r)\n",
    "    init_time = time.time() - start\n",
    "\n",
    "    seed_cost = cost_function(data_rdd, centroidsParallel1)\n",
    "\n",
    "    #final_centroids = miniBatchKMeans(data_rdd, centroidsParallel1, iterations, batch_fraction)\n",
    "    final_centroids = kMeansNaive(data_array, centroidsRandom, iterations)\n",
    "\n",
    "    final_cost = cost_function(data_rdd, final_centroids)\n",
    "\n",
    "    results.append({\n",
    "        \"method\": \"kmeans|| (l=k/2)\",\n",
    "        \"k\": k,\n",
    "        \"l\": 0.5*k,\n",
    "        \"r\": r, \n",
    "        \"initialization_time\": init_time,\n",
    "        \"seed\": seed_cost,\n",
    "        \"final\": final_cost\n",
    "    })\n",
    "\n",
    "\n",
    "    # 4 - Parallel l=2k\n",
    "    start = time.time()\n",
    "    centroidsParallel2 = kMeansParallel_init(data_rdd, k=k, l=2*k, r=r)\n",
    "    init_time = time.time() - start\n",
    "    \n",
    "    seed_cost = cost_function(data_rdd, centroidsParallel2)\n",
    "\n",
    "    #final_centroids = miniBatchKMeans(data_rdd, centroidsParallel2, iterations, batch_fraction)\n",
    "    final_centroids = kMeansNaive(data_array, centroidsRandom, iterations)\n",
    "\n",
    "    final_cost = cost_function(data_rdd, final_centroids)\n",
    "    \n",
    "    results.append({\n",
    "        \"method\": \"kmeans|| (l=2k)\",\n",
    "        \"k\": k,\n",
    "        \"l\": 2*k,\n",
    "        \"r\": r, \n",
    "        \"initialization_time\": init_time,\n",
    "        \"seed\": seed_cost,\n",
    "        \"final\": final_cost\n",
    "    })\n",
    "\n",
    "    df = pd.DataFrame(results)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analysis_fin(\n",
    "        data_array: npt.ArrayLike, \n",
    "        data_rdd: RDD, \n",
    "        r: int, \n",
    "        k: int, \n",
    "        iterations: int=10, \n",
    "        batch_fraction: float=0.1\n",
    "        ) -> npt.DTypeLike:\n",
    "    results = []\n",
    "    \"\"\"\"\n",
    "    Computes performance tracking metrics for different initialization algorithms with *seed cost* (after initialization) and *final cost* (after miniBatchKmean iterations)\n",
    "    \"\"\"\n",
    "    # 1 - Random initialization\n",
    "    start = time.time()\n",
    "    centroidsRandom = data_array[np.random.choice(data_array.shape[0], size=k, replace=False)]\n",
    "    init_time = time.time() - start\n",
    "    seed_cost = cost_function(data_array, centroidsRandom)\n",
    "    \n",
    "    final_centroids = miniBatchKMeans(data_rdd, centroidsRandom, iterations, batch_fraction)\n",
    "    final_cost = cost_function(data_array, final_centroids)\n",
    "    \n",
    "    results.append({\n",
    "        \"method\": \"random\",\n",
    "        \"k\": k, \n",
    "        \"l\": np.nan,\n",
    "        \"r\": r, \n",
    "        \"initialization_time\": init_time,\n",
    "        \"seed\": seed_cost,\n",
    "        \"final\": final_cost\n",
    "    })\n",
    "\n",
    "    # 2 - k-means++\n",
    "    start = time.time()\n",
    "    centroidsPlusPlus = kMeansPlusPlus_init(data_array, k)\n",
    "    init_time = time.time() - start\n",
    "    seed_cost = cost_function(data_array, centroidsPlusPlus)\n",
    "    final_centroids = miniBatchKMeans(data_rdd, centroidsPlusPlus, iterations, batch_fraction)\n",
    "    final_cost = cost_function(data_array, final_centroids)\n",
    "    results.append({\n",
    "        \"method\": \"kmeans++\",\n",
    "        \"k\": k,\n",
    "        \"l\": np.nan,\n",
    "        \"r\": r, \n",
    "        \"initialization_time\": init_time,\n",
    "        \"seed\": seed_cost,\n",
    "        \"final\": final_cost\n",
    "    })\n",
    "\n",
    "    # 3 - Parallel l=0.5k\n",
    "    start = time.time()\n",
    "    centroidsParallel1 = kMeansParallel_init(data_rdd, k=k, l=0.5*k)\n",
    "    init_time = time.time() - start\n",
    "    seed_cost = cost_function(data_array, centroidsParallel1)\n",
    "    final_centroids = miniBatchKMeans(data_rdd, centroidsParallel1, iterations, batch_fraction)\n",
    "    final_cost = cost_function(data_array, final_centroids)\n",
    "    results.append({\n",
    "        \"method\": \"kmeans|| (l=k/2)\",\n",
    "        \"k\": k,\n",
    "        \"l\": 0.5*k,\n",
    "        \"r\": r, \n",
    "        \"initialization_time\": init_time,\n",
    "        \"seed\": seed_cost,\n",
    "        \"final\": final_cost\n",
    "    })\n",
    "\n",
    "    # 4 - Parallel l=2k\n",
    "    start = time.time()\n",
    "    centroidsParallel2 = kMeansParallel_init(data_rdd, k=k, l=2*k)\n",
    "    init_time = time.time() - start\n",
    "    seed_cost = cost_function(data_array, centroidsParallel2)\n",
    "    final_centroids = miniBatchKMeans(data_rdd, centroidsParallel2, iterations, batch_fraction)\n",
    "    final_cost = cost_function(data_array, final_centroids)\n",
    "    results.append({\n",
    "        \"method\": \"kmeans|| (l=2k)\",\n",
    "        \"k\": k,\n",
    "        \"l\": 2*k,\n",
    "        \"r\": r, \n",
    "        \"initialization_time\": init_time,\n",
    "        \"seed\": seed_cost,\n",
    "        \"final\": final_cost\n",
    "    })\n",
    "\n",
    "    df = pd.DataFrame(results)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Paper analysis on Gaussian mixture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R = [1, 10, 100]\n",
    "\n",
    "# generate data with Gaussian mixture changing R\n",
    "for RR in R:\n",
    "    data_rdd = gauss_mixture(num_points_per_cluster=5000, num_clusters=50, dim=15, R=RR)\n",
    "    # Convert to numpy array\n",
    "    data_list = data_rdd.collect()\n",
    "    data_array = np.array(data_list)\n",
    "\n",
    "    df = analysis_init(data_array, data_rdd, r=5, k=50)\n",
    "    print(\"R =\", RR, \"\\n\")\n",
    "    print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stopping the cluster\n",
    "! $SPARK_HOME/sbin/stop-all.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
